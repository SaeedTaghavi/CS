{
    "docs": [
        {
            "location": "/",
            "text": "Operating System | notes\n\u00b6\n\n\nGetting Started\n\u00b6\n\n\nIn this website, I'll work on my notes when I have class of \nOperating System, Spring 2018\n by Professor \nTei-Wei Kuo\n\n\nThe materials are mainly from the \nOperating System Concepts, 9th Edition\n.\n\n\nPlease don't hesitate to give me your feedback if any adjustment is needed with the notes. You can simply press the \"Pencil icon\" in the upper right corner to edit the contents.\n\n\nHow I generate this website\n\u00b6\n\n\nI use the static site generator \nMkDocs\n and the beautiful theme \nMaterial for MkDocs\n to build this website!\n\n\nSince there are some LaTeX equations \nKaTeX\n doesn't support, here I use \nMathJax\n to render the math equations in my website.\n\n\nI also add \noverflow-x: auto\n to prevent the overflow issue on mobile devices, so you can scroll horizontally in the math display equations.\n\n\nMore Informations\n\u00b6\n\n\nFor more informations please visit \nmy github site\n.\n\n\nMy blog: \nJay's Blog\n\n\nMail to: \nwalkccray@gmail.com\n\n\nBy Jay Chen on April 13, 2018.",
            "title": "Preface"
        },
        {
            "location": "/#operating-system-notes",
            "text": "",
            "title": "Operating System | notes"
        },
        {
            "location": "/#getting-started",
            "text": "In this website, I'll work on my notes when I have class of  Operating System, Spring 2018  by Professor  Tei-Wei Kuo  The materials are mainly from the  Operating System Concepts, 9th Edition .  Please don't hesitate to give me your feedback if any adjustment is needed with the notes. You can simply press the \"Pencil icon\" in the upper right corner to edit the contents.",
            "title": "Getting Started"
        },
        {
            "location": "/#how-i-generate-this-website",
            "text": "I use the static site generator  MkDocs  and the beautiful theme  Material for MkDocs  to build this website!  Since there are some LaTeX equations  KaTeX  doesn't support, here I use  MathJax  to render the math equations in my website.  I also add  overflow-x: auto  to prevent the overflow issue on mobile devices, so you can scroll horizontally in the math display equations.",
            "title": "How I generate this website"
        },
        {
            "location": "/#more-informations",
            "text": "For more informations please visit  my github site .  My blog:  Jay's Blog  Mail to:  walkccray@gmail.com  By Jay Chen on April 13, 2018.",
            "title": "More Informations"
        },
        {
            "location": "/Chap01/",
            "text": "Dummy\n\u00b6",
            "title": "Chapter 1 Introduction"
        },
        {
            "location": "/Chap01/#dummy",
            "text": "",
            "title": "Dummy"
        },
        {
            "location": "/Chap02/",
            "text": "Chapter 2 Operating-System Structures\n\u00b6\n\n\nObjectives:\n\n\n\n\n\n\nTo describe the services an operating system provides to users, processes, and other systems.\n\n\n\n\n\n\nTo discuss the various ways of structuring an operating system.\n\n\n\n\n\n\nTo explain how operating systems are installed and customized and how they boot.\n\n\n\n\n\n\n2.1 Operating-System Services\n\u00b6\n\n\n\n\n\n\n\n\nUser interface (UI)\n\n\n\n\ncommand-line interface (CLI)\n\n\nbatch interface\n\n\ngraphical user interface \n\n\n\n\n\n\n\n\nProgram execution. OS load a program into memory -> run that program -> end execution\n\n\n\n\nnormally\n\n\nabnormally (error)\n\n\n\n\n\n\n\n\nI/O operations. A running program may require I/O:\n\n\n\n\nfile\n\n\nI/O device: recording to a CD or DVD ...\n\n\n\n\n\n\n\n\nFile-system manipulation.\n\n\n\n\nread/write files\n\n\ncreate/delete them by name\n\n\nsearch\n\n\nlist file (ls)\n\n\n\n\n\n\n\n\nCommunications.\n\n\n\n\nshared memory\n\n\nmessage passing: packets of information in predefined formats are moved between processes by the operating system\n\n\n\n\n\n\n\n\nError detection\n\n\n\n\n\n\nResource allocation\n\n\n\n\n\n\nAccounting. users can be billed\n\n\n\n\n\n\nProtection and security\n\n\n\n\n\n\n2.2 User and Operating-System Interface\n\u00b6\n\n\n2.2.1 Command Interpreters\n\u00b6\n\n\nOn systems with multiple command interpreters to choose from, the interpreters are known as \nshells\n.\n\n\n\n\n\n\nthe command interpreter itself contains the code to execute the command.\n\n\n\n\n\n\nthe command interpreter merely uses the command to identify a file to be loaded into memory and executed.\n\n\n\n\neg. \nrm\n\n\n\n\n\n\n\n\n2.2.2 Graphical User Interfaces\n\u00b6\n\n\n\n\ndesktop\n\n\nicons\n\n\nfolder\n\n\nmouse\n\n\ngestures on the touchscreen\n\n\n\n\n2.2.3 Choice of Interface\n\u00b6\n\n\n\n\nshell scripts\n\n\neg. \nUNIX\n and \nLinux\n.\n\n\n\n\n\n\n\n\n2.3 Systems Calls\n\u00b6\n\n\n\n\nSystem calls provide an interface to the services made available by an operating system.\n\n\neg. writing a simple program to read data from one file and copy them to another file causes a lot of system calls!\n\n\n\n\nC/C++\n\n\n\n\nEach read and write must return status information regarding various possible error conditions.\n\n\n\n\napplication programming interface (API): it specifies a set of functions\n\n\nWindows API\n\n\nPOSIX API\n\n\nUNIX\n\n\nLinux\n\n\nmacOS\n\n\n\n\n\n\nJava API\n\n\n\n\n\n\n\n\nlibc\n: UNIX and Linux for programs written in C\n\n\nWhy prefer API rather than invoking actual system calls?\n\n\n\n\nprotability (expected to run on any system)\n\n\nactual system calls can be more difficult to learn\n\n\n\n\nThe relationship between an \nAPI\n, the \nsystem-call interface\n, and the \nOS\n\n\n\b\n\n\nThe caller need know nothing about how the system call is implemented or what it does during execution. Rather, the caller need only obey the API and understand what the operating system will do as a result of the execution of that system call.\n\n\nMake explicit to implicit\n\n\nThree general methods are used to pass parameters to the operating system.\n\n\n\n\n\n\nthrough registers (Linux and Solaris)\n\n\n\n\nblock,\n\n\ntable, \n\n\nmemory, \n\n\nand the address of the\n\n\n\n\n\n\n\n\nplaced or pushed onto the stack -> popped off the stack by the OS\n\n\n\n\n\n\n\n\n2.4 Types of System Calls\n\u00b6\n\n\n2.4.1 Process Control\n\u00b6\n\n\nA running program halts either\n\n\n\n\nnormally: \nend()\n\n\nabnormally: \nabort()\n\n\n\n\nerror -> dump (written to disk, may be examined by a debugger)\n\n\nMore severe errors can be indicated by a higher-level error parameter.\n\n\neg. Standard C Library\n\n\n\n\n2.4.2 File Management\n\u00b6\n\n\n2.4.3 Device Management\n\u00b6\n\n\n2.4.4 Information Maintenance\n\u00b6\n\n\nMany systems provide system calls to \ndump()\n memory. This provision is useful for debugging. A program trace lists each system call as it is executed. Even microprocessors provide a CPU mode known as single step, in which a trap is executed by the CPU after every instruction. The trap is usually caught by a debugger.\n\n\n2.4.5 Communication\n\u00b6\n\n\n2.4.6 Protection\n\u00b6\n\n\n2.5 System Programs\n\u00b6\n\n\n2.6 Operating-System Design and Implementation\n\u00b6\n\n\n2.6.1 Design Goals\n\u00b6\n\n\n2.6.2 Mechanisms and Policies\n\u00b6\n\n\n2.6.3 Implementation\n\u00b6",
            "title": "Chapter 2 Operating-System Structures"
        },
        {
            "location": "/Chap02/#chapter-2-operating-system-structures",
            "text": "Objectives:    To describe the services an operating system provides to users, processes, and other systems.    To discuss the various ways of structuring an operating system.    To explain how operating systems are installed and customized and how they boot.",
            "title": "Chapter 2 Operating-System Structures"
        },
        {
            "location": "/Chap02/#21-operating-system-services",
            "text": "User interface (UI)   command-line interface (CLI)  batch interface  graphical user interface      Program execution. OS load a program into memory -> run that program -> end execution   normally  abnormally (error)     I/O operations. A running program may require I/O:   file  I/O device: recording to a CD or DVD ...     File-system manipulation.   read/write files  create/delete them by name  search  list file (ls)     Communications.   shared memory  message passing: packets of information in predefined formats are moved between processes by the operating system     Error detection    Resource allocation    Accounting. users can be billed    Protection and security",
            "title": "2.1 Operating-System Services"
        },
        {
            "location": "/Chap02/#22-user-and-operating-system-interface",
            "text": "",
            "title": "2.2 User and Operating-System Interface"
        },
        {
            "location": "/Chap02/#221-command-interpreters",
            "text": "On systems with multiple command interpreters to choose from, the interpreters are known as  shells .    the command interpreter itself contains the code to execute the command.    the command interpreter merely uses the command to identify a file to be loaded into memory and executed.   eg.  rm",
            "title": "2.2.1 Command Interpreters"
        },
        {
            "location": "/Chap02/#222-graphical-user-interfaces",
            "text": "desktop  icons  folder  mouse  gestures on the touchscreen",
            "title": "2.2.2 Graphical User Interfaces"
        },
        {
            "location": "/Chap02/#223-choice-of-interface",
            "text": "shell scripts  eg.  UNIX  and  Linux .",
            "title": "2.2.3 Choice of Interface"
        },
        {
            "location": "/Chap02/#23-systems-calls",
            "text": "System calls provide an interface to the services made available by an operating system.  eg. writing a simple program to read data from one file and copy them to another file causes a lot of system calls!   C/C++   Each read and write must return status information regarding various possible error conditions.   application programming interface (API): it specifies a set of functions  Windows API  POSIX API  UNIX  Linux  macOS    Java API     libc : UNIX and Linux for programs written in C  Why prefer API rather than invoking actual system calls?   protability (expected to run on any system)  actual system calls can be more difficult to learn   The relationship between an  API , the  system-call interface , and the  OS  \b  The caller need know nothing about how the system call is implemented or what it does during execution. Rather, the caller need only obey the API and understand what the operating system will do as a result of the execution of that system call.  Make explicit to implicit  Three general methods are used to pass parameters to the operating system.    through registers (Linux and Solaris)   block,  table,   memory,   and the address of the     placed or pushed onto the stack -> popped off the stack by the OS",
            "title": "2.3 Systems Calls"
        },
        {
            "location": "/Chap02/#24-types-of-system-calls",
            "text": "",
            "title": "2.4 Types of System Calls"
        },
        {
            "location": "/Chap02/#241-process-control",
            "text": "A running program halts either   normally:  end()  abnormally:  abort()   error -> dump (written to disk, may be examined by a debugger)  More severe errors can be indicated by a higher-level error parameter.  eg. Standard C Library",
            "title": "2.4.1 Process Control"
        },
        {
            "location": "/Chap02/#242-file-management",
            "text": "",
            "title": "2.4.2 File Management"
        },
        {
            "location": "/Chap02/#243-device-management",
            "text": "",
            "title": "2.4.3 Device Management"
        },
        {
            "location": "/Chap02/#244-information-maintenance",
            "text": "Many systems provide system calls to  dump()  memory. This provision is useful for debugging. A program trace lists each system call as it is executed. Even microprocessors provide a CPU mode known as single step, in which a trap is executed by the CPU after every instruction. The trap is usually caught by a debugger.",
            "title": "2.4.4 Information Maintenance"
        },
        {
            "location": "/Chap02/#245-communication",
            "text": "",
            "title": "2.4.5 Communication"
        },
        {
            "location": "/Chap02/#246-protection",
            "text": "",
            "title": "2.4.6 Protection"
        },
        {
            "location": "/Chap02/#25-system-programs",
            "text": "",
            "title": "2.5 System Programs"
        },
        {
            "location": "/Chap02/#26-operating-system-design-and-implementation",
            "text": "",
            "title": "2.6 Operating-System Design and Implementation"
        },
        {
            "location": "/Chap02/#261-design-goals",
            "text": "",
            "title": "2.6.1 Design Goals"
        },
        {
            "location": "/Chap02/#262-mechanisms-and-policies",
            "text": "",
            "title": "2.6.2 Mechanisms and Policies"
        },
        {
            "location": "/Chap02/#263-implementation",
            "text": "",
            "title": "2.6.3 Implementation"
        },
        {
            "location": "/Chap03/",
            "text": "Chapter 3 Process Concept\n\u00b6\n\n\n3.1 Process Concept\n\u00b6\n\n\n\n\nProcess\n\n\nA program in execution, the basis of all computation.\n\n\n\n\n\n\nBatch system: jobs (= process)\n\n\nTime-shared system: user programs or tasks\n\n\n\n\n3.1.1 The process\n\u00b6\n\n\nProcess consists\n\n\n\n\ntext\n section: program code\n\n\ndata\n section: contains \nglobal variables\n\n\nheap\n: memory\n\n\ncurrent activity (\nprogram counter\n + \nregisters\n)\n\n\n\bstack\n: contains \ntemporary data\n:\n\n\nfunction parameters\n\n\nreturn addresses\n\n\n\n\nlocal variables\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProgram\n\n\nProcess\n\n\n\n\n\n\n\n\n\n\npassive\n entity\n\n\nactive\n entity\n\n\n\n\n\n\na file containing a list of instructions stored on disk (executable file)\n\n\nprogram counter: specifying the next instruction to execute + a set of associated resources\n\n\n\n\n\n\n\n\nWhen\n\n\n\n\ndouble-clicking an icon\n\n\nprog.exe\n\n\na.out\n\n\n\n\nan executable file is loaded into memory: \nprogram $\\to$ process\n.\n\n\nTwo different processes: the text section are equivalent, the data, heap and stack vary.\n\n\nProcess can be an execution environment for other code. (\nsimulation\n)\n\n\neg.\n\n\n    \njava\n \ntestProgram\n\n\n\n\n\n\n\nThe command \njava\n runs the JVM as an ordinary process, then executes the Java program \ntestProgram\n in the VM.\n\n\n\n\n3.1.2 Process State\n\u00b6\n\n\n\n\nNew\n.\n\n\nRunning\n. Execute instructions\n\n\nWaiting\n. Wait some event (I/O, signal)\n\n\nReady\n. Wait to be assigned to a processor\n\n\nTerminated\n.\n\n\n\n\n\n\n\n\nProcess and Processor\n\n\nOnly \n1\n process runs on any processor. (Many processes may be \nready\n and \nwaiting\n.)\n\n\n\n\n3.1.3 Process Control Block\n\u00b6\n\n\n\n\nProcess state\n.\n\n\nProgram counter\n. Address of the next instruction.\n\n\nCPU registers\n. Accumulators, index registers, stack pointers, general-purpose registers, and any condition-code information.\n\n\nCPU-scheduling information\n.\n\n\nMemory-management information\n.\n\n\nAccounting information\n. The amount of CPU and real time used, time limits, account numbers, job or process numbers.\n\n\nI/O status information\n. The list of I/O devices allocated to the process, a list of open files.\n\n\n\n\n\n\n3.2 Process Scheduling\n\u00b6\n\n\n\n\nMultiprogramming\n. To have some process running at all times $\\to$ maximize CPU utilization.\n\n\nTime sharing\n. Switch the CPU among processes.\n\n\nProcess scheduler\n. Selects an available process.\n\n\n\n\n3.2.1 Scheduling Queues\n\u00b6\n\n\nAs processes enter the system, they are put into a \njob queue\n.\n\n\nJob queue\n. Consists of all processes in the system.\n\n\nReady queue\n. Keep \nready\n and \nwaiting\n processes.\n\n\n\n\n\n\nWhen a process exit, it is removed from all queues and has its PCB and resources deallocated.\n\n\n3.2.2 Schedulers\n\u00b6\n\n\nProcesses are first spooled to a mass-storage device (eg. disk). Then \n\n\n\n\n\n\nLong-term scheduler\n (job)\n\n\n\n\nselects processes from this pool.\n\n\nloads theme into memory \nfor\n execution.\n\n\n\n\n\n\n\n\nShort-term scheduler\n (CPU)\n\n\n\n\nselects from among the processes that are ready to execute\n\n\nallocates CPU to one of them.\n\n\n\n\n\n\n\n\n\n\nLong-term scheduler\n\n\n\n\nControls the \ndegree of multiprogramming\n (# processes).\n\n\nSelects a good \nprocess mix\n of I/O-bound and CPU-bound.\n\n\n\n\n\n\n\n\nMedium-term scheduler\n\n\nSwapping.\n\n\n\n\n\n\n3.2.3 Context Switch\n\u00b6\n\n\n\n\nWhen a context switch occurs\n\n\nThe kernel saves the context of the old process in its PCB and loads the saved context of the new process scheduled to run.\n\n\n\n\n3.3 Operations on Processes\n\u00b6\n\n\n3.3.1 Process Creation\n\u00b6\n\n\n\n\nProcess Identifier (pid)\n\n\nAn integer number, which provides a unique value for each process in the system, and it can be used as an \nindex\n to access various attributes of a process within the kernel.\n\n\n\n\n\n\ninit process\n\n\nA process has pid = 1, and serves as the root parent process for all user processes.\n\n\n\n\n\n\nWhen a process creates a child process, that child process may obtain the resources from\n\n\n\n\nOS\n\n\na subset of parent process\n\n\n\n\nWhen a process creates a new process\n\n\n\n\nThe parent continues to execute concurrently with its children.\n\n\nThe parent waits until some or all of its children have terminated.\n\n\n\n\nThere are also two address-space possibilities for the new process\n\n\n\n\nThe child process is a duplicate of the parent process (it has the same program and data as the parent).\n\n\nThe child process has a new program loaded into it.\n\n\n\n\n\n\nfork()\n\n\nThe new process created by \nfork()\n consists of a copy of the address of parent process.\n\n\n\n\nReturn code\n\n\n\n\nChild process: 0.\n\n\nParent process: pid of the child.\n\n\n\n\n\n\nAfter \nfork()\n syscall\n\n\nOne of the two processes uses the \nexec()\n syscall to replace the process's memory space with a new program.\n\n\n\n\nCreating a separate process using the UNIX \nfork()\n system call.\n\n\nint\n \nmain\n()\n \n{\n \n    \npid\n \nt\n \npid\n;\n\n\n    \n/* fork a child process */\n\n    \npid\n \n=\n \nfork\n();\n\n    \nif\n \n(\npid\n \n<\n \n0\n)\n \n{\n                      \n/* error occurred */\n\n        \nfprintf\n(\nstderr\n,\n \n\"Fork Failed\"\n);\n\n        \nreturn\n \n1\n;\n\n    \n}\n \nelse\n \nif\n \n(\npid\n \n==\n \n0\n)\n \n{\n              \n/* child process */\n\n        \nexeclp\n(\n\"/bin/ls\"\n,\n \n\"ls\"\n,\n \nNULL\n);\n  \n/* a version of the `exec()` */\n\n    \n}\n \nelse\n \n{\n                            \n/* parent process */\n\n        \n/* parent will wait for the child to complete */\n\n        \nwait\n(\nNULL\n);\n\n        \nprintf\n(\n\"Child Complete\"\n);\n\n    \n}\n\n    \nreturn\n \n0\n;\n\n\n}\n\n\n\n\n\n\n3.3.2 Process Termination\n\u00b6\n\n\nA process terminates when it finishes executing its final statement and asks the operating system to delete it by using the \nexit()\n system call.\n\n\n\n\nTerminating process\n\n\nA parent needs to know the identities of its children if it is to terminate them.\n\n\n\n\nA parent can terminate its children by\n\n\n\n\nThe child use too much resources. (The parent have a mechanism to inspect the state of its children.)\n\n\nThe task assigned to the child is no longer required.\n\n\nThe parent is exiting.\n\n\n\n\n\n\nCascading Termination\n\n\nIf a process terminates (either normally or abnormally), then all its children must also be terminated.\n\n\n\n\nexit()\n may be called either directly or indirectly (\nreturn\n):\n\n\n    \nexit\n(\n1\n);\n    \n/* directly exit with status 1 */\n\n\n\n\n\n\n\nProcess Table Entry (PTE)\n\n\nContains the process's exit status.\n\n\n\n\n\n\nZombie\n\n\nA process terminated, but whose parent hasn't called \nwait()\n. Once the parent calls \nwait()\n, the pid of the zombie process and its entry in the PTE are released.\n\n\n\n\nThe \ninit\n process periodically invokes \nwait()\n to collect and release the orphan's pid and PTE.\n\n\n3.4 Interprocess Communication\n\u00b6\n\n\nProcesses have two classifications:\n\n\n\n\nIndependent\n\n\nCooperating\n\n\nInformation sharing\n\n\nComputation speedup - multicore\n\n\nModularity\n\n\nConvenience - parallel tasks\n\n\n\n\n\n\n\n\nInterprocess communication (IPC)\n\n\n\n\nShared memory: slower (syscalls are required.)\n\n\nMessage passing: faster (syscalls are required only to establish shared memory regions.)\n\n\n\n\n\n\n3.4.1 Shared-Memory Systems\n\u00b6\n\n\nProducer\u2013consumer problem\n\u00b6\n\n\nA \nproducer\n process produces information that is consumed by a \nconsumer\n process.\n\n\neg.\n\n\n\n\nA compiler produce assembly code that is consumed by an assembler. The assembler, in turn, may produce object modules that are consumed by the loader.\n\n\nA server as a producer and a client as a consumer.\n\n\n\n\nWe need a buffer which resides in a region of shared memory (producer & consumer), and can be filled by the producer and emptied by the consumer.\n\n\n\n\nUnbounded buffer\n\n\nBounded buffer (more practical)\n\n\n\n\nImplement the shared \nbuffer\n as a circular array.\n\n#define BUFFER_SIZE 10\n\n\n\ntypedef\n \nstruct\n \n{\n\n    \n...\n\n\n}\n \nitem\n;\n\n\n\nitem\n \nbuffer\n[\nBUFFER_SIZE\n];\n\n\nint\n \nin\n \n=\n \n0\n;\n     \n/* points to the next free position */\n\n\nint\n \nout\n \n=\n \n0\n;\n    \n/* points to the first full position */\n\n\n\n\nwhile\n \n(\ntrue\n)\n \n{\n\n    \n/* produce an item in next_produced */\n\n\n    \nwhile\n \n(((\nin\n \n+\n \n1\n)\n \n%\n \nBUFFER_SIZE\n)\n \n==\n \nout\n)\n\n        \n;\n \n/* do nothing */\n\n\n    \nbuffer\n[\nin\n]\n \n=\n \nnext_produced\n;\n\n    \nin\n \n=\n \n(\nin\n \n+\n \n1\n)\n \n%\n \nBUFFER_SIZE\n;\n\n\n}\n\n\n\n\n\nitem\n \nnext_consumed\n;\n\n\n\nwhile\n \n(\ntrue\n)\n \n{\n\n    \nwhile\n \n(\nin\n \n==\n \nout\n)\n\n        \n;\n \n/* do nothing */\n\n\n        \nnext_consumed\n \n=\n \nbuffer\n[\nout\n];\n\n        \nout\n \n=\n \n(\nout\n \n+\n \n1\n)\n \n%\n \nBUFFER_SIZE\n;\n\n\n        \n/* consume the item in next_consumed */\n\n\n}\n\n\n\n\n\n3.4.2 Message-Passing Systems\n\u00b6\n\n\nMessage passing provides a mechanism to allow processes to communicate and to synchronize their actions \nwithout sharing the same address space\n.\n\n\n\n\nCommunication link\n\n\nIf processes $P$ and $Q$ want to communicate, they must send messages to and receive messages from each other.\n\n\n\n\nSeveral implentation of \nsend()\n/\nreceive()\n operations:\n- Direct of indirect communication\n- Synchronous or asynchronous communication\n- Automatic or explicit buffering\n\n\n3.4.2.1 Naming\n\u00b6\n\n\n\n\n\n\nDirect communication\n\n\nThe messages are sent to and received from processes.\n\n\n\n\n\n\nSymmetry ()\n\n\n\n\nsend(P, message)\n\n\nreceive(Q, message)\n\n\n\n\n\n\n\n\nAsymmetry\n\n\n\n\nsend(P, message)\n\n\nreceive(id, message)\n\n\n\n\n\n\n\n\n\n\n\n\nIndirect communication\n\n\nThe messages are sent to and received from \nmailboxes\n, or \nports\n.\n\n\n\n\nsend(A, message)\n \u2014 Send a message to mailbox A.\n\n\nreceive(A, message)\n \u2014 Receive a message from mailbox A.\n\n\n\n\n\n\n\n\nThe process that creates a new mailbox is that mailbox's owner by default.\n\n\n\n\nA mailbox can be owned by the OS.\n\n\n\n\n3.4.2.2 Synchronization\n\u00b6\n\n\nMessage passing may be either\n\n\n\n\n\n\nBlocking (synchronous)\n\n\n\n\nBlocking send. (blocked until the message is received)\n\n\nBlocking receive.\n\n\n\n\n\n\n\n\nNonblocking (asynchronous)\n\n\n\n\nNonblocking send.\n\n\nNonblocking receive. (valid message or a null)\n\n\n\n\n\n\n\n\n\n\nRendezvous\n\n\nWhen both \nsend()\n and \nreceive()\n are blocking.\n\n\n\n\n3.4.2.3 Buffering\n\u00b6\n\n\nMessages reside in a temporary queue:\n\n\n\n\nZero capacity. (no buffering)\n\n\nBounded capacity.\n\n\nUnbounded capacity.\n\n\n\n\n3.5 Examples of IPC Systems\n\u00b6\n\n\n3.5.1 An Example: POSIX Shared Memory\n\u00b6\n\n\nmessage\n \nnext_consumed\n;\n\n\n\nwhile\n \n(\ntrue\n)\n \n{\n\n    \nreceive\n(\nnext_consumed\n);\n\n\n    \n/* consume the item in next consumed */\n\n\n}\n\n\n\n\n\nA process must first create a shared-memory:\n\n\n    \nshm_fd\n \n=\n \nshm_open\n(\nname\n,\n \nO_CREAT\n \n|\n \nO_RDRW\n,\n \n0666\n);\n\n\n\n\n\nThe \nftruncate()\n function configure the size of the object in bytes:\n\n\n    \nftruncate\n(\nshm_fd\n,\n \n4096\n);\n\n\n\n\n\n3.5.2 An Example: Mach\n\u00b6\n\n\nEven system calls are made by messages. When a task is created, two special mailboxes\n\n\n\n\nkernel mailbox\n\n\nnotify mailbox\n\n\n\n\nare also created.\n\n\nThere are three syscalls needed:\n\n\n\n\n\n\nmsg_send()\n\n\nIf the mailbox is full:\n\n\n\n\nWait indefinitely until there is room in the mailbox.\n\n\nWait at most $n$ milliseconds.\n\n\nDo not wait at all but rather return immediately.\n\n\nTemporarily cache a message. (server tasks)\n\n\n\n\n\n\n\n\nmsg_receive()\n\n\n\n\nmsg_rpc()\n: sends a message and waits for exactly one return message from the sender.\n\n\n\n\n\n\nRemote\n\n\nThe RPC (Remote Procedure Call) models a typical subroutine procedure call but can work between systems.\n\n\n\n\n\n\nport_allocate()\n\n\nCreates a new mailbox and allocates space for its queue of messages.\n\n\n\n\nMach guarantees that multiple messages from the same sender are queued in first-in, first-out (FIFO) order but does not guarantee an absolute ordering.\n\n\n\n\nOne task can either own or receive from a mailbox\n\n\n\n\n\n\nMailbox set\n\n\nA collection of mailboxes.\n\n\n\n\n\n\nport_status()\n\n\neg. # of messages in a mailbox.\n\n\n\n\n3.5.3 An Example: Windows\n\u00b6\n\n\nApplication programs can be considered clients of a subsystem server.\n\n\n\n\nAdvanced Local Procedure Call (ALPC)\n\n\nIt is used for communication between two processes \non the same machine\n.\n\n\n\n\nWindows uses two types of ports\n\n\n\n\nConnection ports\n\n\nCommunication ports\n\n\n\n\n\n\nCallback\n\n\nAllows the client and server to accept requests when they would normally be expecting a reply.\n\n\n\n\nWhen an ALPC channel is created, 1 of 3 message-passing techniques is chosen:\n\n\n\n\nSmall messages: using the port's message queue.\n\n\nLarger messages: passed through a \nsection object\n (a region of shared memory.)\n\n\nVery large messages: calling API to read/write directly into the address space.\n\n\n\n\n\n\n3.6 Communication in Client\u2013Server Systems\n\u00b6\n\n\n3.6.1 Sockets\n\u00b6\n\n\n\n\nSocket\n\n\nAn endpoint for communication. (IP + port#)\n\n\n\n\nA pair of processes communicating over a network employs \na pair\n of sockets\u2014one for each process.\n\n\n\n\nSocket behavior\n\n\nThe server waits for incoming client requests by listening to a specified port. Once a request is received, the server accepts a connection from the client socket to complete the connection.\n\n\n\n\nWell-known ports: (all ports below 1024 are considered well known)\n\n\n\n\n23: telnet\n\n\n21: FTP\n\n\n80: HTTP\n\n\n\n\n\n\nJava provides:\n\n\n\n\nConnection-oriented (TCP) sockets: \nSocket\n.\n\n\nConnectionless (UDP) sockets: \nDatagramSocket\n.\n\n\nMulticastSocket\n: a subclass of \nDatagramSocket\n. It allows data to be sent to multiple recipients.\n\n\n\n\n\n\nLoopback\n\n\nIP address 127.0.0.1.\n\n\n\n\n3.6.2 Remote Procedure Calls\n\u00b6\n\n\nThe RPC was designed as a way to abstract the procedure-call mechanism for use between systems with network connections.\n\n\nEach message is addressed to an RPC daemon listening to a port on the remote system, and each contains an identifier specifying the function to execute and the parameters to pass to that function.\n\n\n\n\nThe semantics of RPCs allows a client to invoke a procedure on a remote host as it would invoke a procedure locally.\n\n\n\n\n\n\nStub\n\n\nThe RPC system hides the details that allow communication to take place by providing a stub on the client side.\n\n\n\n\n\n\nParameter marshalling\n\n\nPackaging the parameters into a form that can be transmitted over a network.\n\n\n\n\nProcedure of RPCs:\n\n\n\n\nThe client invokes a RPC\n\n\nRPC system\n\n\ncalls the appropriate stub (client side).\n\n\npasses the stub the parameters to the RPC.\n\n\n\n\n\n\nMarshals parameter: packaging the parameters into a form that can be transmitted over a network.\n\n\nThe stub transmits a message to the server using message passing.\n\n\nA stub (server side) \n\n\nreceives this message\n\n\ninvokes the procedure on the server.\n\n\n\n\n\n\n(optional) Return values using the same technique.\n\n\n\n\nIssues for RPC:\n\n\n\n\nData representation\n\n\nExternal Data Representation (XDR)\n\n\nParameter marshalling\n\n\n\n\n\n\n\n\n\n\nSemantics of a call\n\n\nat most once\n\n\nexactly once (ACK)\n\n\n\n\n\n\nBinding of the client and server port\n\n\nMatchmaker (a rendezvous mechanism)\n\n\n\n\n\n\n\n\n\n\n3.6.3 Pipes\n\u00b6\n\n\nIn implementing a pipe, four issues:\n\n\n\n\nDoes the pipe allow bidirectional communication, or is communication unidirectional?\n\n\nIf two-way communication is allowed, is it half duplex (data can travel only one way at a time) or full duplex (data can travel in both directions at the same time)?\n\n\nMust a relationship (such as parent\u2013child) exist between the communicating processes?\n\n\nCan the pipes communicate over a network, or must the communicating processes reside on the same machine?\n\n\n\n\n3.6.3.1 Ordinary Pipes\n\u00b6\n\n\n    \npipe\n(\nint\n \nfd\n[])\n\n\n\n\n\n\n\nOrdinarya pipes on on Windows: \nanonymous pipes\n (similar to UNIX.)\n\n\n3.6.3.2 Named Pipes\n\u00b6\n\n\n\n\n\n\n\n\nOrdinary Pipes\n\n\nNamed Pipes\n\n\n\n\n\n\n\n\n\n\nunidirectional\n\n\nbidirectional\n\n\n\n\n\n\nparent-child required\n\n\nnot required\n\n\n\n\n\n\n\n\n\n\nIn UNIX, named pipes = FIFOs. A FIFO is created with the \nmkfifo()\n.\n\n\n\n\nPipes in practice:\n\n\n \n# In this scenario, the ls command serves as the producer, and its output is consumed by the more command.\n\n $ ls \n|\n more",
            "title": "Chapter 3 Processes"
        },
        {
            "location": "/Chap03/#chapter-3-process-concept",
            "text": "",
            "title": "Chapter 3 Process Concept"
        },
        {
            "location": "/Chap03/#31-process-concept",
            "text": "Process  A program in execution, the basis of all computation.    Batch system: jobs (= process)  Time-shared system: user programs or tasks",
            "title": "3.1 Process Concept"
        },
        {
            "location": "/Chap03/#311-the-process",
            "text": "Process consists   text  section: program code  data  section: contains  global variables  heap : memory  current activity ( program counter  +  registers )  \bstack : contains  temporary data :  function parameters  return addresses   local variables          Program  Process      passive  entity  active  entity    a file containing a list of instructions stored on disk (executable file)  program counter: specifying the next instruction to execute + a set of associated resources     When   double-clicking an icon  prog.exe  a.out   an executable file is loaded into memory:  program $\\to$ process .  Two different processes: the text section are equivalent, the data, heap and stack vary.  Process can be an execution environment for other code. ( simulation )  eg.       java   testProgram    The command  java  runs the JVM as an ordinary process, then executes the Java program  testProgram  in the VM.",
            "title": "3.1.1 The process"
        },
        {
            "location": "/Chap03/#312-process-state",
            "text": "New .  Running . Execute instructions  Waiting . Wait some event (I/O, signal)  Ready . Wait to be assigned to a processor  Terminated .     Process and Processor  Only  1  process runs on any processor. (Many processes may be  ready  and  waiting .)",
            "title": "3.1.2 Process State"
        },
        {
            "location": "/Chap03/#313-process-control-block",
            "text": "Process state .  Program counter . Address of the next instruction.  CPU registers . Accumulators, index registers, stack pointers, general-purpose registers, and any condition-code information.  CPU-scheduling information .  Memory-management information .  Accounting information . The amount of CPU and real time used, time limits, account numbers, job or process numbers.  I/O status information . The list of I/O devices allocated to the process, a list of open files.",
            "title": "3.1.3 Process Control Block"
        },
        {
            "location": "/Chap03/#32-process-scheduling",
            "text": "Multiprogramming . To have some process running at all times $\\to$ maximize CPU utilization.  Time sharing . Switch the CPU among processes.  Process scheduler . Selects an available process.",
            "title": "3.2 Process Scheduling"
        },
        {
            "location": "/Chap03/#321-scheduling-queues",
            "text": "As processes enter the system, they are put into a  job queue .  Job queue . Consists of all processes in the system.  Ready queue . Keep  ready  and  waiting  processes.    When a process exit, it is removed from all queues and has its PCB and resources deallocated.",
            "title": "3.2.1 Scheduling Queues"
        },
        {
            "location": "/Chap03/#322-schedulers",
            "text": "Processes are first spooled to a mass-storage device (eg. disk). Then     Long-term scheduler  (job)   selects processes from this pool.  loads theme into memory  for  execution.     Short-term scheduler  (CPU)   selects from among the processes that are ready to execute  allocates CPU to one of them.      Long-term scheduler   Controls the  degree of multiprogramming  (# processes).  Selects a good  process mix  of I/O-bound and CPU-bound.     Medium-term scheduler  Swapping.",
            "title": "3.2.2 Schedulers"
        },
        {
            "location": "/Chap03/#323-context-switch",
            "text": "When a context switch occurs  The kernel saves the context of the old process in its PCB and loads the saved context of the new process scheduled to run.",
            "title": "3.2.3 Context Switch"
        },
        {
            "location": "/Chap03/#33-operations-on-processes",
            "text": "",
            "title": "3.3 Operations on Processes"
        },
        {
            "location": "/Chap03/#331-process-creation",
            "text": "Process Identifier (pid)  An integer number, which provides a unique value for each process in the system, and it can be used as an  index  to access various attributes of a process within the kernel.    init process  A process has pid = 1, and serves as the root parent process for all user processes.    When a process creates a child process, that child process may obtain the resources from   OS  a subset of parent process   When a process creates a new process   The parent continues to execute concurrently with its children.  The parent waits until some or all of its children have terminated.   There are also two address-space possibilities for the new process   The child process is a duplicate of the parent process (it has the same program and data as the parent).  The child process has a new program loaded into it.    fork()  The new process created by  fork()  consists of a copy of the address of parent process.   Return code   Child process: 0.  Parent process: pid of the child.    After  fork()  syscall  One of the two processes uses the  exec()  syscall to replace the process's memory space with a new program.   Creating a separate process using the UNIX  fork()  system call.  int   main ()   {  \n     pid   t   pid ; \n\n     /* fork a child process */ \n     pid   =   fork (); \n     if   ( pid   <   0 )   {                        /* error occurred */ \n         fprintf ( stderr ,   \"Fork Failed\" ); \n         return   1 ; \n     }   else   if   ( pid   ==   0 )   {                /* child process */ \n         execlp ( \"/bin/ls\" ,   \"ls\" ,   NULL );    /* a version of the `exec()` */ \n     }   else   {                              /* parent process */ \n         /* parent will wait for the child to complete */ \n         wait ( NULL ); \n         printf ( \"Child Complete\" ); \n     } \n     return   0 ;  }",
            "title": "3.3.1 Process Creation"
        },
        {
            "location": "/Chap03/#332-process-termination",
            "text": "A process terminates when it finishes executing its final statement and asks the operating system to delete it by using the  exit()  system call.   Terminating process  A parent needs to know the identities of its children if it is to terminate them.   A parent can terminate its children by   The child use too much resources. (The parent have a mechanism to inspect the state of its children.)  The task assigned to the child is no longer required.  The parent is exiting.    Cascading Termination  If a process terminates (either normally or abnormally), then all its children must also be terminated.   exit()  may be called either directly or indirectly ( return ):       exit ( 1 );      /* directly exit with status 1 */    Process Table Entry (PTE)  Contains the process's exit status.    Zombie  A process terminated, but whose parent hasn't called  wait() . Once the parent calls  wait() , the pid of the zombie process and its entry in the PTE are released.   The  init  process periodically invokes  wait()  to collect and release the orphan's pid and PTE.",
            "title": "3.3.2 Process Termination"
        },
        {
            "location": "/Chap03/#34-interprocess-communication",
            "text": "Processes have two classifications:   Independent  Cooperating  Information sharing  Computation speedup - multicore  Modularity  Convenience - parallel tasks     Interprocess communication (IPC)   Shared memory: slower (syscalls are required.)  Message passing: faster (syscalls are required only to establish shared memory regions.)",
            "title": "3.4 Interprocess Communication"
        },
        {
            "location": "/Chap03/#341-shared-memory-systems",
            "text": "",
            "title": "3.4.1 Shared-Memory Systems"
        },
        {
            "location": "/Chap03/#producerconsumer-problem",
            "text": "A  producer  process produces information that is consumed by a  consumer  process.  eg.   A compiler produce assembly code that is consumed by an assembler. The assembler, in turn, may produce object modules that are consumed by the loader.  A server as a producer and a client as a consumer.   We need a buffer which resides in a region of shared memory (producer & consumer), and can be filled by the producer and emptied by the consumer.   Unbounded buffer  Bounded buffer (more practical)   Implement the shared  buffer  as a circular array. #define BUFFER_SIZE 10  typedef   struct   { \n     ...  }   item ;  item   buffer [ BUFFER_SIZE ];  int   in   =   0 ;       /* points to the next free position */  int   out   =   0 ;      /* points to the first full position */   while   ( true )   { \n     /* produce an item in next_produced */ \n\n     while   ((( in   +   1 )   %   BUFFER_SIZE )   ==   out ) \n         ;   /* do nothing */ \n\n     buffer [ in ]   =   next_produced ; \n     in   =   ( in   +   1 )   %   BUFFER_SIZE ;  }   item   next_consumed ;  while   ( true )   { \n     while   ( in   ==   out ) \n         ;   /* do nothing */ \n\n         next_consumed   =   buffer [ out ]; \n         out   =   ( out   +   1 )   %   BUFFER_SIZE ; \n\n         /* consume the item in next_consumed */  }",
            "title": "Producer\u2013consumer problem"
        },
        {
            "location": "/Chap03/#342-message-passing-systems",
            "text": "Message passing provides a mechanism to allow processes to communicate and to synchronize their actions  without sharing the same address space .   Communication link  If processes $P$ and $Q$ want to communicate, they must send messages to and receive messages from each other.   Several implentation of  send() / receive()  operations:\n- Direct of indirect communication\n- Synchronous or asynchronous communication\n- Automatic or explicit buffering",
            "title": "3.4.2 Message-Passing Systems"
        },
        {
            "location": "/Chap03/#3421-naming",
            "text": "Direct communication  The messages are sent to and received from processes.    Symmetry ()   send(P, message)  receive(Q, message)     Asymmetry   send(P, message)  receive(id, message)       Indirect communication  The messages are sent to and received from  mailboxes , or  ports .   send(A, message)  \u2014 Send a message to mailbox A.  receive(A, message)  \u2014 Receive a message from mailbox A.     The process that creates a new mailbox is that mailbox's owner by default.   A mailbox can be owned by the OS.",
            "title": "3.4.2.1 Naming"
        },
        {
            "location": "/Chap03/#3422-synchronization",
            "text": "Message passing may be either    Blocking (synchronous)   Blocking send. (blocked until the message is received)  Blocking receive.     Nonblocking (asynchronous)   Nonblocking send.  Nonblocking receive. (valid message or a null)      Rendezvous  When both  send()  and  receive()  are blocking.",
            "title": "3.4.2.2 Synchronization"
        },
        {
            "location": "/Chap03/#3423-buffering",
            "text": "Messages reside in a temporary queue:   Zero capacity. (no buffering)  Bounded capacity.  Unbounded capacity.",
            "title": "3.4.2.3 Buffering"
        },
        {
            "location": "/Chap03/#35-examples-of-ipc-systems",
            "text": "",
            "title": "3.5 Examples of IPC Systems"
        },
        {
            "location": "/Chap03/#351-an-example-posix-shared-memory",
            "text": "message   next_consumed ;  while   ( true )   { \n     receive ( next_consumed ); \n\n     /* consume the item in next consumed */  }   A process must first create a shared-memory:       shm_fd   =   shm_open ( name ,   O_CREAT   |   O_RDRW ,   0666 );   The  ftruncate()  function configure the size of the object in bytes:       ftruncate ( shm_fd ,   4096 );",
            "title": "3.5.1 An Example: POSIX Shared Memory"
        },
        {
            "location": "/Chap03/#352-an-example-mach",
            "text": "Even system calls are made by messages. When a task is created, two special mailboxes   kernel mailbox  notify mailbox   are also created.  There are three syscalls needed:    msg_send()  If the mailbox is full:   Wait indefinitely until there is room in the mailbox.  Wait at most $n$ milliseconds.  Do not wait at all but rather return immediately.  Temporarily cache a message. (server tasks)     msg_receive()   msg_rpc() : sends a message and waits for exactly one return message from the sender.    Remote  The RPC (Remote Procedure Call) models a typical subroutine procedure call but can work between systems.    port_allocate()  Creates a new mailbox and allocates space for its queue of messages.   Mach guarantees that multiple messages from the same sender are queued in first-in, first-out (FIFO) order but does not guarantee an absolute ordering.   One task can either own or receive from a mailbox    Mailbox set  A collection of mailboxes.    port_status()  eg. # of messages in a mailbox.",
            "title": "3.5.2 An Example: Mach"
        },
        {
            "location": "/Chap03/#353-an-example-windows",
            "text": "Application programs can be considered clients of a subsystem server.   Advanced Local Procedure Call (ALPC)  It is used for communication between two processes  on the same machine .   Windows uses two types of ports   Connection ports  Communication ports    Callback  Allows the client and server to accept requests when they would normally be expecting a reply.   When an ALPC channel is created, 1 of 3 message-passing techniques is chosen:   Small messages: using the port's message queue.  Larger messages: passed through a  section object  (a region of shared memory.)  Very large messages: calling API to read/write directly into the address space.",
            "title": "3.5.3 An Example: Windows"
        },
        {
            "location": "/Chap03/#36-communication-in-clientserver-systems",
            "text": "",
            "title": "3.6 Communication in Client\u2013Server Systems"
        },
        {
            "location": "/Chap03/#361-sockets",
            "text": "Socket  An endpoint for communication. (IP + port#)   A pair of processes communicating over a network employs  a pair  of sockets\u2014one for each process.   Socket behavior  The server waits for incoming client requests by listening to a specified port. Once a request is received, the server accepts a connection from the client socket to complete the connection.   Well-known ports: (all ports below 1024 are considered well known)   23: telnet  21: FTP  80: HTTP    Java provides:   Connection-oriented (TCP) sockets:  Socket .  Connectionless (UDP) sockets:  DatagramSocket .  MulticastSocket : a subclass of  DatagramSocket . It allows data to be sent to multiple recipients.    Loopback  IP address 127.0.0.1.",
            "title": "3.6.1 Sockets"
        },
        {
            "location": "/Chap03/#362-remote-procedure-calls",
            "text": "The RPC was designed as a way to abstract the procedure-call mechanism for use between systems with network connections.  Each message is addressed to an RPC daemon listening to a port on the remote system, and each contains an identifier specifying the function to execute and the parameters to pass to that function.   The semantics of RPCs allows a client to invoke a procedure on a remote host as it would invoke a procedure locally.    Stub  The RPC system hides the details that allow communication to take place by providing a stub on the client side.    Parameter marshalling  Packaging the parameters into a form that can be transmitted over a network.   Procedure of RPCs:   The client invokes a RPC  RPC system  calls the appropriate stub (client side).  passes the stub the parameters to the RPC.    Marshals parameter: packaging the parameters into a form that can be transmitted over a network.  The stub transmits a message to the server using message passing.  A stub (server side)   receives this message  invokes the procedure on the server.    (optional) Return values using the same technique.   Issues for RPC:   Data representation  External Data Representation (XDR)  Parameter marshalling      Semantics of a call  at most once  exactly once (ACK)    Binding of the client and server port  Matchmaker (a rendezvous mechanism)",
            "title": "3.6.2 Remote Procedure Calls"
        },
        {
            "location": "/Chap03/#363-pipes",
            "text": "In implementing a pipe, four issues:   Does the pipe allow bidirectional communication, or is communication unidirectional?  If two-way communication is allowed, is it half duplex (data can travel only one way at a time) or full duplex (data can travel in both directions at the same time)?  Must a relationship (such as parent\u2013child) exist between the communicating processes?  Can the pipes communicate over a network, or must the communicating processes reside on the same machine?",
            "title": "3.6.3 Pipes"
        },
        {
            "location": "/Chap03/#3631-ordinary-pipes",
            "text": "pipe ( int   fd [])    Ordinarya pipes on on Windows:  anonymous pipes  (similar to UNIX.)",
            "title": "3.6.3.1 Ordinary Pipes"
        },
        {
            "location": "/Chap03/#3632-named-pipes",
            "text": "Ordinary Pipes  Named Pipes      unidirectional  bidirectional    parent-child required  not required      In UNIX, named pipes = FIFOs. A FIFO is created with the  mkfifo() .   Pipes in practice:    # In this scenario, the ls command serves as the producer, and its output is consumed by the more command. \n $ ls  |  more",
            "title": "3.6.3.2 Named Pipes"
        },
        {
            "location": "/Chap04/",
            "text": "Chapter 4 Threads\n\u00b6\n\n\n4.1 Overview\n\u00b6\n\n\n\n\nThread\u2013Lightweight process (LWP)\n\n\nA basit unit of CPU utilization.\n\n\n\n\nA thread shares\n\n\n\n\ncode section\n\n\ndata section\n\n\nOS resources (eg. open files and signals) \n\n\n\n\nA thread have its own\n\n\n\n\nthread ID\n\n\nprogram counter\n\n\nregister set\n\n\nstack\n\n\n\n\n\n\n4.1.1 Motivation\n\u00b6\n\n\nIt is generally more efficient to use one process that contains multiple threads since process creation is time consuming and resource intensive.\n\n\n\n\n4.1.2 Benefits\n\u00b6\n\n\nThe benefits of multithreaded:\n\n\n\n\nResponsiveness\n\n\nResource sharing\n\n\nEconomy\n\n\nScalability/Utilization\n\n\n\n\n4.2 Multicore Programming\n\u00b6\n\n\nA more recent, similar trend in system design is to place multiple computing cores on a single chip.\n\n\n\n\nMulticore or Multiprocessor systems\n\n\nThe cores appear across CPU chips or within CPU chips.\n\n\n\n\nConsider an application with 4 threads.\n\n\n\n\n\n\nWith a single core\n\n\n\n\n\n\n\n\nWith multiple cores\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nParallelism\n\n\nConcurrency\n\n\n\n\n\n\n\n\n\n\nPerform more than one task simultaneously.\n\n\nAllow all the tasks to make progress.\n\n\n\n\n\n\n\n\n\n\nAmdahl's Law\n\n\nIf $S$ is the portion cannot be accelerated by $N$ cores (serially).\n\n\n$$speedup \\le \\frac{1}{S + \\frac{(1 - S)}{N}}$$\n\n\n\n\n4.2.1 Programming Challenges\n\u00b6\n\n\n\n\nIdentifying tasks: Dividing Activities\n\n\nBalance (Equal value)\n\n\nData splitting\n\n\nData dependency\n\n\nTesting and debugging\n\n\n\n\n4.2.2 Types of Parallelism\n\u00b6\n\n\n\n\nData parallelism\n\n\nDistribute subsets of the same data across multiple computing cores.\n\n\nEach core performs the same operation.\n\n\n\n\neg. \n\n\n$$\\sum_{i = 0}^{N - 1} arr[i] = \\sum_{i = 0}^{N / 2 - 1} arr[i] (\\text{thread } A) + \\sum_{i = N / 2}^{N - 1} arr[i] (\\text{thread } B).$$\n\n\n\n\nTask parallelism\n\n\nDistribute tasks (threads) across multiple computing cores.\n\n\nEach thread performs a unique operation.\n\n\n\n\n4.3 Multithreading Models\n\u00b6\n\n\n4.3.1 Many-to-One Model\n\u00b6\n\n\n\n\n\n\npros:\n\n\n\n\nEfficiency\n\n\n\n\n\n\n\n\ncons:\n\n\n\n\nOne blocking syscall blocks all.\n\n\nNo parallelism for multiple processors.\n\n\n\n\n\n\n\n\neg. Green threads (Solaris)\n\n\n\n\n4.3.2 One-to-One Model\n\u00b6\n\n\n\n\n\n\npros:\n\n\n\n\nOne syscall blocks one thread.\n\n\n\n\n\n\n\n\ncons:\n\n\n\n\nOverheads in creating a kernel thread.\n\n\n\n\n\n\n\n\neg. Windows NT/2000/XP, Linux, OS/2, Solaris 9\n\n\n\n\n4.3.3 Many-to-Many Model\n\u00b6\n\n\n\n\npros:\n\n\nA combination of parallelism and efficiency.\n\n\n\n\n\n\n\n\neg. Solaris 2 & 9, IRIX, HP-UX, Tru64 UNIX\n\n\n\n\n\n\n4.4 Thread Libraries\n\u00b6\n\n\n\n\nUser level\n\n\nKernel level\n\n\n\n\nThree main thread libraries:\n\n\n\n\nPOSIX Pthreads: User or kernel level\n\n\nWindows: Kernel level\n\n\nJava: Level depending on the thread library on the host system.\n\n\n\n\nTwo general strategie for creating threads:\n\n\n\n\nAsynchronous threading: parent doesn't know children.\n\n\nSynchronous threading: parent must wait for all of its children. (\nfork-join\n)\n\n\n\n\nAll of the following examples use synchronous threading.\n\n\n4.4.1 Pthreads\n\u00b6\n\n\n\n\nPthreads\n\n\nThe POSIX standard (IEEE 1003.1c) defining an API for thread creation and synchronization. This is a \nspecification\n for thread behavior, not and \nimplementation\n.\n\n\n\n\n4.4.2 Windows Threads\n\u00b6\n\n\n4.4.3 Java Threads\n\u00b6\n\n\nCreating a \nThread\n object does not specifically create the new thread. $\\to$ \nstart()\n does!\n\n\n\n\nIt allocates memory and initializes a new thread in the JVM.\n\n\nIt calls the \nrun()\n method, making the thread eligible to be run by the JVM.\n(Note again that we never call the \nrun()\n method directly. Rather, we call the \nstart()\n method, and it calls the \nrun()\n method on our behalf.)\n\n\n\n\n4.5 Implicit Threading\n\u00b6\n\n\n4.5.1 Thread Pools\n\u00b6\n\n\nThe issue of multithreaded server:\n\n\n\n\nTime to create the thread\n\n\nConcurrency\n\n\n\n\n\n\nThread pool\n\n\nCreate a number of threads at process startup and place them into a pool, where they sit and wait for work.\n\n\n\n\nThe benefits of thread pools:\n\n\n\n\nSpeed\n\n\nLimited # of threads, which is good for OS.\n\n\nSeperating the task of creating tasks allows us to use different strategies.\n\n\nDynamic or static thread pools\n\n\n\n\neg. \nQueueUserWorkItem()\n, \njava.util.concurrent\n.\n\n\n4.5.2 OpenMP\n\u00b6\n\n\n\n\nOpenMP\n\n\nA set of compiler directivese and APIs to support parallel programming in shared memory environment.\n\n\nThreads with divided workload are created automatically based on # of cores or a set bound.\n\n\n\n\n\n\nParallel regions\n\n\nBlocks of code that may run in parallel.\n\n\n\n\nWhen OpenMP encounters\n\n\n    \n#pragma omp parallel\n\n\n\n\n\nit creates as many threads are there are processing cores in the system.\n\n\neg.\n\n\n#pragma omp parallel for\n\n\nfor\n \n(\ni\n \n=\n \n0\n;\n \ni\n \n<\n \nN\n;\n \ni\n++\n)\n\n    \nc\n[\ni\n]\n \n=\n \na\n[\ni\n]\n \n+\n \nb\n[\ni\n];\n\n\n\n\n\n4.5.3 Grand Central Dispatch\n\u00b6\n\n\nA ma\nsOS/iOS combination of extensions to the C.\n\n\nLike OpenMP, GCD manges most of the details of threading.\n\n\n    \n^\n{\n \nprintf\n(\n\"I am a block.\"\n);\n \n}\n\n\n\n\n\nGCD schedules blocks for run-time execution by placing them on a \ndispatch queue\n.\n\n\n\n\nSerial (FIFO)\n\n\nConcurrent (FIFO)\n\n\nlow\n\n\ndefault\n\n\nhigh\n\n\n\n\n\n\n\n\ndispatch_queue_t\n \nqueue\n \n=\n \ndispatch_get_global_queue\n(\nDISPATCH_QUEUE_PRIORITY_DEFAULT\n,\n \n0\n);\n\n\ndispatch_async\n(\nqueue\n,\n \n^\n{\n \nprintf\n(\n\"I am a block.\"\n)\n \n});\n\n\n\n\n\n4.5.4 Other Approaches\n\u00b6\n\n\neg. Intel's Threading Building Blocks (TBB), \njava.util.concurrent\n.\n\n\n4.6 Threading Issues\n\u00b6\n\n\n4.6.1 The \nfork()\n and \nexec()\n System Calls\n\u00b6\n\n\n\n\nfork()\n issue\n\n\n\n\nDuplicate all threads?\n\n\nIs the new process single-threaded?\n\n\n\n\n\n\n\n\nexec()\n issue\n\n\nIf a thread invokese the \nexec()\n, the program specified in the parameter to \nexec()\n will replace the entire process\u2014including all threads. Thus if \nexec()\n is called immediately after forking, then duplicating all threads is unnecessary.\n\n\n\n\n4.6.2 Signal Handling\n\u00b6\n\n\nAll signals follows:\n\n\n\n\nA signal is generated by the occurrence of a particular event.\n\n\nThe signal is delivered to a process.\n\n\nOnce delivered, the signal must be handled.\n\n\n\n\nTwo types of signals:\n\n\n\n\n\n\nSynchronous signal: delivered to the same process that performed the operation causing the signal.\n\n\n\n\nillegal memory access\n\n\ndivision by 0\n\n\n\n\n\n\n\n\nAsynchronous signal\n\n\n\n\ngenerated by an external process (eg. ^C)\n\n\nhaving a timer expire\n\n\n\n\n\n\n\n\nA signal may be handled by:\n\n\n\n\nA default signal handler\n\n\nA user-defined signal handler\n\n\n\n\nSignal delivering:\n\n\n\n\nDeliver the signal to the thread to which the signal applies. (eg. division by 0)\n\n\nDeliver the signal to every thread in the process. (eg. ^C)\n\n\nDeliver the signal to certain threads in the process.\n\n\nAssign a specific thread to receive all signals for the process.\n\n\n\n\nFunctions/Methods for delivering a signal:\n\n\n\n\n\n\nUNIX:\n\n\n    \nkill\n(\npid_t\n \npid\n,\n \nint\n \nsignal\n)\n\n\n\n\n\n\n\n\n\nPOSIX:\n\n\n    \npthread_kill\n(\npthread_t\n \ntid\n,\n \nint\n \nsignal\n)\n\n\n\n\n\n\n\n\n\nWindows:\n\n\n\n\nAsynchronous Procedure Calls (APCs)\n\n\n\n\n\n\n\n\n4.6.3 Thread Cancellation\n\u00b6\n\n\n\n\nTarget thread\n\n\nA thread that is to be canceled.\n\n\n\n\nCancellation of a target thread may occur in two different scenarios:\n\n\n\n\nAsynchronous cancellation\n. One thread immediately terminates the target thread.\n\n\nDeferred cancellation\n. The target thread periodically checks whether it should terminate, allowing it an opportunity to terminate itself in an orderly fashion.\n\n\n\n\n\n\nCanceling a thread asynchronously may not free a necessary system-wide resource.\n\n\n\n\npthread_tid\n;\n\n\n\n/* create the thread */\n\n\npthread_create\n(\n&\ntid\n,\n \n0\n,\n \nworker\n,\n \nNULL\n);\n\n\n...\n\n\n/* cancel the thread */\n\n\npthread_cancel\n(\ntid\n);\n\n\n\n\n\nPthreads supports three cancellation modes:\n\n\n\n\n\n\n\n\nMode\n\n\nState\n\n\nType\n\n\n\n\n\n\n\n\n\n\nOff\n\n\nDisabled\n\n\n\u2013\n\n\n\n\n\n\nDeferred\n\n\nEnabled\n\n\nDeferred\n\n\n\n\n\n\nAsynchronous\n\n\nEnabled\n\n\nAsynchronous\n\n\n\n\n\n\n\n\n\n\nCancellation point\n\n\nCancellation occurs only when a thread reaches a cancellation point.\n\n\n\n\n\n\nCleanup handler\n\n\nIf a cancellation request is found to be pending, this function allows any resources a thread may have acquired to be released before the thread is terminated.\n\n\n\n\neg. deferred cancellation:\n\n\nwhile\n \n(\n1\n)\n \n{\n\n    \n/* do some work for awhile */\n\n\n    \n/* check if there is a cancellation request */\n\n    \npthread_testcancel\n();\n\n\n}\n\n\n\n\n\n4.6.4 Thread-Local Storage\n\u00b6\n\n\n\n\nThread-Local Storage (TLS)\n\n\nEach thread might need its own copy of certain data.\n\n\n\n\nTLS is similar to \nstatic\n data.\n\n\n\n\n\n\n\n\nTLS\n\n\nLocal variables\n\n\n\n\n\n\n\n\n\n\nvisible across function invocations\n\n\nvisible only during a single function invocation\n\n\n\n\n\n\n\n\n4.6.5 Scheduler Activations\n\u00b6\n\n\n\n\nLightweight process (LWP)\n\n\nA virtual processor (kernel threads) on which the application can schedule a user thread to run. (many-to-many or two-level)\n\n\n\n\n\n\n\n\nScheduler activation\n\n\nThe kernel provides an application with a set of virtual processors (LWPs), and the application can schedule user threads onto an available virtual processor.\n\n\n\n\n\n\nUpcall\n\n\nThe kernel must inform an application about certain events.\n\n\n\n\n4.7 Operating-System Examples\n\u00b6\n\n\n4.7.1 Windows Threads\n\u00b6\n\n\nThe general components of a thread include:\n\n\n\n\nA thread ID uniquely identifying the thread\n\n\nA register set representing the status of the processor\n\n\nA user stack, employed when the thread is running in user mode\n\n\nA kernel stack, employed when the thread is running in kernel mode\n\n\nA private storage area used by various run-time libraries and dynamic link libraries (DLLs)\n\n\n\n\nContext\n (register set, stacks, and private sotrage area) of the thread:\n\n\n\n\n\n\nKernel space\n\n\n\n\n\n\nETHREAD\u2014executive thread block:\n\n\n\n\na pointer to the process\n\n\nthe address of the routine\n\n\na pointer to the corresponding KTHREAD\n\n\n\n\n\n\n\n\nKTHREAD\u2014kernel thread block\n\n\n\n\nscheduling/synchronization information\n\n\nthe kernel stack\n\n\na pointer to TEB\n\n\n\n\n\n\n\n\n\n\n\n\nUser space\n\n\n\n\nTEB\u2014thread environment block\n\n\nthe thread ID\n\n\na user-mode stack\n\n\nan array for TLS\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n4.7.2 Linux Threads\n\u00b6\n\n\nclone()\n vs \nfork()\n\n\n\n\nterm \ntask\n\u2014rather than \nprocess\n or \nthread\n\n\nSeveral per-process data structures\n\n\npoints\n to the same data structures for open files, signal handling, virtual memory, etc.",
            "title": "Chapter 4 Threads"
        },
        {
            "location": "/Chap04/#chapter-4-threads",
            "text": "",
            "title": "Chapter 4 Threads"
        },
        {
            "location": "/Chap04/#41-overview",
            "text": "Thread\u2013Lightweight process (LWP)  A basit unit of CPU utilization.   A thread shares   code section  data section  OS resources (eg. open files and signals)    A thread have its own   thread ID  program counter  register set  stack",
            "title": "4.1 Overview"
        },
        {
            "location": "/Chap04/#411-motivation",
            "text": "It is generally more efficient to use one process that contains multiple threads since process creation is time consuming and resource intensive.",
            "title": "4.1.1 Motivation"
        },
        {
            "location": "/Chap04/#412-benefits",
            "text": "The benefits of multithreaded:   Responsiveness  Resource sharing  Economy  Scalability/Utilization",
            "title": "4.1.2 Benefits"
        },
        {
            "location": "/Chap04/#42-multicore-programming",
            "text": "A more recent, similar trend in system design is to place multiple computing cores on a single chip.   Multicore or Multiprocessor systems  The cores appear across CPU chips or within CPU chips.   Consider an application with 4 threads.    With a single core     With multiple cores        Parallelism  Concurrency      Perform more than one task simultaneously.  Allow all the tasks to make progress.      Amdahl's Law  If $S$ is the portion cannot be accelerated by $N$ cores (serially).  $$speedup \\le \\frac{1}{S + \\frac{(1 - S)}{N}}$$",
            "title": "4.2 Multicore Programming"
        },
        {
            "location": "/Chap04/#421-programming-challenges",
            "text": "Identifying tasks: Dividing Activities  Balance (Equal value)  Data splitting  Data dependency  Testing and debugging",
            "title": "4.2.1 Programming Challenges"
        },
        {
            "location": "/Chap04/#422-types-of-parallelism",
            "text": "Data parallelism  Distribute subsets of the same data across multiple computing cores.  Each core performs the same operation.   eg.   $$\\sum_{i = 0}^{N - 1} arr[i] = \\sum_{i = 0}^{N / 2 - 1} arr[i] (\\text{thread } A) + \\sum_{i = N / 2}^{N - 1} arr[i] (\\text{thread } B).$$   Task parallelism  Distribute tasks (threads) across multiple computing cores.  Each thread performs a unique operation.",
            "title": "4.2.2 Types of Parallelism"
        },
        {
            "location": "/Chap04/#43-multithreading-models",
            "text": "",
            "title": "4.3 Multithreading Models"
        },
        {
            "location": "/Chap04/#431-many-to-one-model",
            "text": "pros:   Efficiency     cons:   One blocking syscall blocks all.  No parallelism for multiple processors.     eg. Green threads (Solaris)",
            "title": "4.3.1 Many-to-One Model"
        },
        {
            "location": "/Chap04/#432-one-to-one-model",
            "text": "pros:   One syscall blocks one thread.     cons:   Overheads in creating a kernel thread.     eg. Windows NT/2000/XP, Linux, OS/2, Solaris 9",
            "title": "4.3.2 One-to-One Model"
        },
        {
            "location": "/Chap04/#433-many-to-many-model",
            "text": "pros:  A combination of parallelism and efficiency.     eg. Solaris 2 & 9, IRIX, HP-UX, Tru64 UNIX",
            "title": "4.3.3 Many-to-Many Model"
        },
        {
            "location": "/Chap04/#44-thread-libraries",
            "text": "User level  Kernel level   Three main thread libraries:   POSIX Pthreads: User or kernel level  Windows: Kernel level  Java: Level depending on the thread library on the host system.   Two general strategie for creating threads:   Asynchronous threading: parent doesn't know children.  Synchronous threading: parent must wait for all of its children. ( fork-join )   All of the following examples use synchronous threading.",
            "title": "4.4 Thread Libraries"
        },
        {
            "location": "/Chap04/#441-pthreads",
            "text": "Pthreads  The POSIX standard (IEEE 1003.1c) defining an API for thread creation and synchronization. This is a  specification  for thread behavior, not and  implementation .",
            "title": "4.4.1 Pthreads"
        },
        {
            "location": "/Chap04/#442-windows-threads",
            "text": "",
            "title": "4.4.2 Windows Threads"
        },
        {
            "location": "/Chap04/#443-java-threads",
            "text": "Creating a  Thread  object does not specifically create the new thread. $\\to$  start()  does!   It allocates memory and initializes a new thread in the JVM.  It calls the  run()  method, making the thread eligible to be run by the JVM.\n(Note again that we never call the  run()  method directly. Rather, we call the  start()  method, and it calls the  run()  method on our behalf.)",
            "title": "4.4.3 Java Threads"
        },
        {
            "location": "/Chap04/#45-implicit-threading",
            "text": "",
            "title": "4.5 Implicit Threading"
        },
        {
            "location": "/Chap04/#451-thread-pools",
            "text": "The issue of multithreaded server:   Time to create the thread  Concurrency    Thread pool  Create a number of threads at process startup and place them into a pool, where they sit and wait for work.   The benefits of thread pools:   Speed  Limited # of threads, which is good for OS.  Seperating the task of creating tasks allows us to use different strategies.  Dynamic or static thread pools   eg.  QueueUserWorkItem() ,  java.util.concurrent .",
            "title": "4.5.1 Thread Pools"
        },
        {
            "location": "/Chap04/#452-openmp",
            "text": "OpenMP  A set of compiler directivese and APIs to support parallel programming in shared memory environment.  Threads with divided workload are created automatically based on # of cores or a set bound.    Parallel regions  Blocks of code that may run in parallel.   When OpenMP encounters       #pragma omp parallel   it creates as many threads are there are processing cores in the system.  eg.  #pragma omp parallel for  for   ( i   =   0 ;   i   <   N ;   i ++ ) \n     c [ i ]   =   a [ i ]   +   b [ i ];",
            "title": "4.5.2 OpenMP"
        },
        {
            "location": "/Chap04/#453-grand-central-dispatch",
            "text": "A ma\nsOS/iOS combination of extensions to the C.  Like OpenMP, GCD manges most of the details of threading.       ^ {   printf ( \"I am a block.\" );   }   GCD schedules blocks for run-time execution by placing them on a  dispatch queue .   Serial (FIFO)  Concurrent (FIFO)  low  default  high     dispatch_queue_t   queue   =   dispatch_get_global_queue ( DISPATCH_QUEUE_PRIORITY_DEFAULT ,   0 );  dispatch_async ( queue ,   ^ {   printf ( \"I am a block.\" )   });",
            "title": "4.5.3 Grand Central Dispatch"
        },
        {
            "location": "/Chap04/#454-other-approaches",
            "text": "eg. Intel's Threading Building Blocks (TBB),  java.util.concurrent .",
            "title": "4.5.4 Other Approaches"
        },
        {
            "location": "/Chap04/#46-threading-issues",
            "text": "",
            "title": "4.6 Threading Issues"
        },
        {
            "location": "/Chap04/#461-the-fork-and-exec-system-calls",
            "text": "fork()  issue   Duplicate all threads?  Is the new process single-threaded?     exec()  issue  If a thread invokese the  exec() , the program specified in the parameter to  exec()  will replace the entire process\u2014including all threads. Thus if  exec()  is called immediately after forking, then duplicating all threads is unnecessary.",
            "title": "4.6.1 The fork() and exec() System Calls"
        },
        {
            "location": "/Chap04/#462-signal-handling",
            "text": "All signals follows:   A signal is generated by the occurrence of a particular event.  The signal is delivered to a process.  Once delivered, the signal must be handled.   Two types of signals:    Synchronous signal: delivered to the same process that performed the operation causing the signal.   illegal memory access  division by 0     Asynchronous signal   generated by an external process (eg. ^C)  having a timer expire     A signal may be handled by:   A default signal handler  A user-defined signal handler   Signal delivering:   Deliver the signal to the thread to which the signal applies. (eg. division by 0)  Deliver the signal to every thread in the process. (eg. ^C)  Deliver the signal to certain threads in the process.  Assign a specific thread to receive all signals for the process.   Functions/Methods for delivering a signal:    UNIX:       kill ( pid_t   pid ,   int   signal )     POSIX:       pthread_kill ( pthread_t   tid ,   int   signal )     Windows:   Asynchronous Procedure Calls (APCs)",
            "title": "4.6.2 Signal Handling"
        },
        {
            "location": "/Chap04/#463-thread-cancellation",
            "text": "Target thread  A thread that is to be canceled.   Cancellation of a target thread may occur in two different scenarios:   Asynchronous cancellation . One thread immediately terminates the target thread.  Deferred cancellation . The target thread periodically checks whether it should terminate, allowing it an opportunity to terminate itself in an orderly fashion.    Canceling a thread asynchronously may not free a necessary system-wide resource.   pthread_tid ;  /* create the thread */  pthread_create ( & tid ,   0 ,   worker ,   NULL );  ...  /* cancel the thread */  pthread_cancel ( tid );   Pthreads supports three cancellation modes:     Mode  State  Type      Off  Disabled  \u2013    Deferred  Enabled  Deferred    Asynchronous  Enabled  Asynchronous      Cancellation point  Cancellation occurs only when a thread reaches a cancellation point.    Cleanup handler  If a cancellation request is found to be pending, this function allows any resources a thread may have acquired to be released before the thread is terminated.   eg. deferred cancellation:  while   ( 1 )   { \n     /* do some work for awhile */ \n\n     /* check if there is a cancellation request */ \n     pthread_testcancel ();  }",
            "title": "4.6.3 Thread Cancellation"
        },
        {
            "location": "/Chap04/#464-thread-local-storage",
            "text": "Thread-Local Storage (TLS)  Each thread might need its own copy of certain data.   TLS is similar to  static  data.     TLS  Local variables      visible across function invocations  visible only during a single function invocation",
            "title": "4.6.4 Thread-Local Storage"
        },
        {
            "location": "/Chap04/#465-scheduler-activations",
            "text": "Lightweight process (LWP)  A virtual processor (kernel threads) on which the application can schedule a user thread to run. (many-to-many or two-level)     Scheduler activation  The kernel provides an application with a set of virtual processors (LWPs), and the application can schedule user threads onto an available virtual processor.    Upcall  The kernel must inform an application about certain events.",
            "title": "4.6.5 Scheduler Activations"
        },
        {
            "location": "/Chap04/#47-operating-system-examples",
            "text": "",
            "title": "4.7 Operating-System Examples"
        },
        {
            "location": "/Chap04/#471-windows-threads",
            "text": "The general components of a thread include:   A thread ID uniquely identifying the thread  A register set representing the status of the processor  A user stack, employed when the thread is running in user mode  A kernel stack, employed when the thread is running in kernel mode  A private storage area used by various run-time libraries and dynamic link libraries (DLLs)   Context  (register set, stacks, and private sotrage area) of the thread:    Kernel space    ETHREAD\u2014executive thread block:   a pointer to the process  the address of the routine  a pointer to the corresponding KTHREAD     KTHREAD\u2014kernel thread block   scheduling/synchronization information  the kernel stack  a pointer to TEB       User space   TEB\u2014thread environment block  the thread ID  a user-mode stack  an array for TLS",
            "title": "4.7.1 Windows Threads"
        },
        {
            "location": "/Chap04/#472-linux-threads",
            "text": "clone()  vs  fork()   term  task \u2014rather than  process  or  thread  Several per-process data structures  points  to the same data structures for open files, signal handling, virtual memory, etc.",
            "title": "4.7.2 Linux Threads"
        },
        {
            "location": "/Chap05/",
            "text": "Chapter 5 Process Synchronization\n\u00b6\n\n\n5.1 Background\n\u00b6\n\n\nRecall \nproducer\u2013consumer problem\n. We modify it as follows:\n\n\nwhile\n \n(\ntrue\n)\n \n{\n\n    \n/* produce an item in next_produced */\n\n\n    \nwhile\n \n(\ncounter\n \n==\n \nBUFFER_SIZE\n)\n \n;\n    \n// do nothing\n\n\n    \nbuffer\n[\nin\n]\n \n=\n \nnext_produced\n;\n\n    \nin\n \n=\n \n(\nin\n \n+\n \n1\n)\n \n%\n \nBUFFER_SIZE\n;\n\n    \ncounter\n++\n;\n\n\n}\n\n\n\n\n\nwhile\n \n(\ntrue\n)\n \n{\n\n    \nwhile\n \n(\ncounter\n \n==\n \n0\n)\n \n;\n      \n// do nothing\n\n\n    \nnext_consumed\n \n=\n \nbuffer\n[\nout\n];\n\n    \nout\n \n=\n \n(\nout\n \n+\n \n1\n)\n \n%\n \nBUFFER_SIZE\n;\n\n    \ncounter\n--\n;\n\n\n    \n/* consume the item in next_consumed */\n\n\n}\n\n\n\n\n\nSuppose \ncounter == 5\n initially. After executing \ncounter++\n in producer or \ncounter--\n in consumer. The value of \ncounter\n may be $4$, $5$, or $6$!\n\n\n\\begin{array}{lllll}\nT_0: producer & \\text{execute} & register_1 = \\text{counter} & \\{register_1 = 5\\} \\\\\nT_1: producer & \\text{execute} & register_1 = register_1 + 1 & \\{register_1 = 6\\} \\\\\nT_2: consumer & \\text{execute} & register_2 = \\text{counter} & \\{register_2 = 5\\} \\\\\nT_3: consumer & \\text{execute} & register_2 = register_2 - 1 & \\{register_2 = 4\\} \\\\\nT_4: producer & \\text{execute} & \\text{counter} = register_1 & \\{counter = 6\\} \\\\\nT_5: consumer & \\text{execute} & \\text{counter} = register_2 & \\{counter = 4\\}\n\\end{array}\n\n\n\n\nRace condition\n\n\nSeveral processes access and manipulate the same data concurrently and the outcome of the execution depends on the \nparticular order\n in which the access takes place.\n\n\n\n\nWe need to ensure that only one process at a time can be manipulating the variable \ncounter\n.\n\n\n5.2 The Critical-Section Problem\n\u00b6\n\n\n\n\nCritical section\n\n\nIn which the process may be changing common variables, updating a table, writing a file, and so on.\n\n\n\n\ndo\n \n{\n\n    \n/* entry section */\n\n        \n/* critical section */\n\n    \n/* exit section */\n\n        \n/* remainder section */\n\n\n}\n \nwhile\n \n(\ntrue\n);\n\n\n\n\n\nA solution to the critical-section problem must satisfy:\n\n\n\n\nMutual exclusion\n\n\nProgree\n. Cannot be postponed indefinitely.\n\n\nBounded waiting\n\n\n\n\nTwo general approches are used to handle critical sections:\n\n\n\n\nPreemptive kernels\n\n\nNonpreemptive kernels\n\n\n\n\nWhy anyone favor a preemptive kernel over a nonpreemptive one?\n\n\n\n\nResponsive.\n\n\nMore suitable for real-time programming.\n\n\n\n\n5.3 Peterson's Solution\n\u00b6\n\n\nPeterson's solution is restricted to two processes that alternate execution between their critical sections and remainder sections. The processes are named $P_i$ and $P_j$.\n\n\nShared data items:\n\n\nint\n \nturn\n;\n\n\nboolean\n \nflag\n[\n2\n];\n\n\n\n\n\ndo\n \n{\n\n    \nflag\n[\ni\n]\n \n=\n \ntrue\n;\n\n    \nturn\n \n=\n \nj\n;\n\n    \nwhile\n \n(\nflag\n[\nj\n]\n \n&&\n \nturn\n \n==\n \nj\n)\n \n;\n\n    \n/* critical section */\n\n\n    \nflag\n[\ni\n]\n \n=\n \nfalse\n;\n\n    \n/* remainder section */\n\n\n}\n \nwhile\n \n(\ntrue\n);\n\n\n\n\n\n\n\nIf both processes try to enter at the same time, \nturn\n will be set to both $i$ and $j$ at roughly the same time. \nOnly one of these assignments will last.\n\n\n\n\nProof\n\n\n\n\nMutual exclusion is preserved.\n\n\nThe progress requirement is satisfied.\n\n\n\n\nThe bounded-waiting requirement is met.\n\n\nSuppose $P_i$ execute $turn = j$ first, then $P_j$ execute $turn = i$. In this assumption, $P_i$ will enter its critical section first and $P_j$ will be stocked in the \nwhile(flag[i] && turn == i\n (remember that here \ni\n should be thinked as \nj\n in the code!). After $P_i$ entering exit section, there are two possibilities:\n\n\n\n\n$P_i$ sets \nflag[i] = false\n, then $P_j$ enters its critical section.\n\n\nAfter $P_i$ setting \nflag[i] = false\n, it immediately sets \nflag[i] = true\n again, consequently, it'll set \nturn = j\n, thus $P_j$ still can enter its critical section.\n\n\n\n\n\n\n\n\nBakery Algorithm\n\u00b6\n\n\n\n\nOriginally designed for distrubuted systems\n\n\nProcesses which are ready to enter their critical section must take a number and wait till the number becomes the lowest.\n\n\n\n\nint\n \nnumber\n[\ni\n];\n          \n// Pi's number if it is nonzeros\n\n\nboolean\n \nchoosing\n[\ni\n];\n    \n// Pi is taking a number\n\n\n\n\n\ndo\n \n{\n\n    \nchoosing\n[\ni\n]\n \n=\n \ntrue\n;\n         \n// A process want to enter its critical section\n\n    \nnumber\n[\ni\n]\n \n=\n \nmax\n(\nnumber\n[\n0\n],\n \n...,\n \nnumber\n[\nn\n \n-\n \n1\n])\n \n+\n \n1\n;\n\n    \nchoosing\n[\nj\n]\n \n=\n \nfalse\n;\n        \n// A process has got its number\n\n    \nfor\n \n(\nj\n \n=\n \n0\n;\n \nj\n \n<\n \nn\n;\n \nj\n++\n)\n \n{\n\n        \nwhile\n \n(\nchoosing\n[\nj\n])\n \n;\n\n        \nwhile\n \n(\nnumber\n[\nj\n]\n \n!=\n \n0\n \n&&\n \n(\nnumber\n[\nj\n],\n \nj\n)\n \n<\n \n(\nnumber\n[\ni\n],\n \ni\n))\n \n;\n     \n// If two processes got the same number, then we should compare their indices\n\n    \n}\n\n    \n/* critical section */\n\n\n    \nnumber\n[\ni\n]\n \n=\n \n0\n;\n\n    \n/* remainder section */\n\n\n}\n \nwhite\n \n(\ntrue\n);\n\n\n\n\n\n\n\nAn observation: If $P_i$ is in its critical section, and $P_k (k != i)$ , then $(number[i], i) < (number[k], k)$.\n\n\n\n\nProof\n\n\n\n\nMutual exclusion: Only the process holds the lowest number can enter the critical section. For each process, when that process doens't get its number, the original process will be stocked in the first while-loop. After that process getting its number, we still need to compare their $numbers$ and $indices$.\n\n\nProgress requirement: The processes won't be forever postponed.\n\n\nBounded-waiting: Assume that a process holds the biggest number, it should wait other processes in the second while-loop. But after all other process entering their exit section and again entering their entry section, they'll get a bigger number, thus the process won't wait forever.\n\n\n\n\n5.4 Synchronization Hardware\n\u00b6\n\n\nDisaple interrupt $\\to$ No preemption:\n\n\n\n\nInfeasible in multiprocessor environment.\n\n\nPotential impacts on interrupt-driven system clocks.\n\n\n\n\n\n\nAtomic\n\n\nModern computer allow us either to test and modify the content of a word or to swap the contents of two words \natomically\n\u2014that is, as one uninterruptible.\n\n\n\n\nAlthough following algorithms satisfy the mutual-exclusion requirement, they don't satisfy the bounded-waiting requirement.\n\n\nboolean\n \ntest_and_set\n(\nboolean\n \n*\ntarget\n)\n \n{\n\n    \nboolean\n \nrv\n \n=\n \n*\ntarget\n;\n\n    \n*\ntarget\n \n=\n \ntrue\n;\n\n\n    \nreturn\n \nrv\n;\n\n\n}\n\n\n\n\n\ndo\n \n{\n\n    \nwhile\n \n(\ntest_and_set\n(\n&\nlock\n))\n \n;\n\n    \n/* critical section */\n\n\n    \nlock\n \n=\n \nfalse\n;\n\n    \n/* remainder section */\n\n\n}\n \nwhile\n \n(\ntrue\n);\n\n\n\n\n\nThe first process executing \nwhile (test_and_set(&lock))\n will set the address value of \nlock\n to \ntrue\n and get the return value \nrv = false\n, thus it won't be stocked in the while-loop and it can enter its critical section.\n\n\n\n\nMutual exclusion: OK\n\n\nProgress requirement: OK\n\n\n\n\nBounded-waiting: FAIL\n\n\nAssume there is only one CPU, after $P_i$ entering its critical section, $P_j$ will be stocked in the while-loop. After $P_i$ exiting its critical section, there are two possibilities:\n\n\n\n\n$P_i$ sets \nlock = false\n, the CPU context switch to $P_j$, thus $P_j$ can enters its critical section.\n\n\nAfter $P_i$ setting \nlock = false\n, the CPU still executes the code of $P_i$, thus $P_i$ enters its critical section again, so $P_j$ may wait forever.\n\n\n\n\n\n\n\n\nvoid\n \nswap\n(\nboolean\n \n*\na\n,\n \nboolean\n \n*\nb\n)\n \n{\n\n    \nboolean\n \ntemp\n \n=\n \n*\na\n;\n\n    \n*\na\n \n=\n \n*\nb\n;\n\n    \n*\nb\n \n=\n \ntemp\n;\n\n\n}\n\n\n\n\n\ndo\n \n{\n\n    \nkey\n \n=\n \ntrue\n;\n\n    \nwhile\n \n(\nkey\n \n==\n \ntrue\n)\n\n        \nswap\n(\n&\nlock\n,\n \n&\nkey\n);\n\n    \n/* critical section */\n\n\n    \nlock\n \n=\n \nfalse\n;\n\n    \n/* remainder section */\n\n\n}\n \nwhile\n \n(\ntrue\n);\n\n\n\n\n\n\n\nMutual exclusion: OK\n\n\nProgress requirement: OK\n\n\nBounded-waiting: FAIL (the reason is like above)\n\n\n\n\nint\n \ncompare_and_swap\n(\nint\n \n*\nvalue\n,\n \nint\n \nexpected\n,\n \nint\n \nnew_value\n)\n \n{\n\n    \nint\n \ntemp\n \n=\n \n*\nvalue\n;\n\n\n    \nif\n \n(\n*\nvalue\n \n==\n \nexpected\n)\n\n        \n*\nvalue\n \n=\n \nnew_value\n;\n\n\n    \nreturn\n \ntemp\n;\n\n\n}\n\n\n\n\n\ndo\n \n{\n\n    \nwhile\n \n(\ncompare_and_swap\n(\n&\nlock\n,\n \n0\n,\n \n1\n)\n \n!=\n \n0\n)\n \n;\n\n    \n/* critical section */\n\n\n    \nlock\n \n=\n \n0\n;\n\n    \n/* remainder section */\n\n\n}\n \nwhile\n \n(\ntrue\n);\n\n\n\n\n\nFollowing algorithms satisfies all the critical-section requirements.\n\n\nboolean\n \nwaiting\n[\nn\n];\n\n\nboolean\n \nlock\n;\n\n\n\n\n\ndo\n \n{\n\n    \nwaiting\n[\ni\n]\n \n=\n \ntrue\n;\n\n    \nkey\n \n=\n \ntrue\n;\n\n    \nwhile\n \n(\nwaiting\n[\ni\n]\n \n&&\n \nkey\n)\n\n        \nkey\n \n=\n \ntest_and_set\n(\n&\nlock\n);\n\n    \nwaiting\n[\ni\n]\n \n=\n \nfalse\n;\n\n    \n/* critical section */\n\n\n    \nj\n \n=\n \n(\ni\n \n+\n \n1\n)\n \n%\n \nn\n;\n                    \n// Assign its next process\n\n    \nwhile\n \n((\nj\n \n!=\n \ni\n)\n \n&&\n \n!\nwaiting\n[\nj\n])\n     \n// Find a following process who is waiting\n\n        \nj\n \n=\n \n(\nj\n \n+\n \n1\n)\n \n%\n \nn\n;\n\n\n    \nif\n \n(\nj\n \n==\n \ni\n)\n                         \n// If no process is waiting\n\n        \nlock\n \n=\n \nfalse\n;\n\n    \nelse\n\n        \nwaiting\n[\nj\n]\n \n=\n \nfalse\n;\n             \n// Thus line 4 will be false and Pj won't be stocked anymore\n\n    \n/* remainder section */\n\n\n}\n \nwhile\n \n(\ntrue\n);\n\n\n\n\n\nAssume \nlock\n is initialized to \nfalse\n.\n\n\n\n\nMutual exclusion: If many processes set their \nwaiting[i] = true\n, after the first process execute \nkey = test_and_set(&lock)\n, \nkey\n will be set to \nfalse\n and \nlock\n will be set to \ntrue\n. Therefore, other processes will be stocked in \nwhile (waiting[i] && key)\n since their \nkey\n will be set to \ntrue\n after \ntest_and_set(&lock)\n (\nlock\n is now \ntrue\n).\n\n\nProgress requirement: Only the process first run \ntest_and_set\n can enter its critical section.\n\n\nBounded-waiting: Wait at most $n - 1$ times.\n\n\n\n\nMutex Locks\n\u00b6\n\n\nA high-level software solution to provide protect critical sections with mutual exclusion.\n\n\n\n\nAcomic execution of \nacquire()\n and \nrelease()\n.\n\n\nSpinlock:\n\n\npros: No context switch for multiprocessor systems.\n\n\ncons: Busy waiting.\n\n\n\n\n\n\n\n\nacquire\n()\n \n{\n\n    \nwhile\n \n(\n!\navailable\n)\n \n;\n        \n// busy wait\n\n    \navailable\n \n=\n \nfalse\n;\n\n\n}\n\n\n\n\n\nrelease\n()\n \n{\n\n    \navailable\n \n=\n \ntrue\n;\n\n\n}\n\n\n\n\n\ndo\n \n{\n\n    \n// acquire lock\n\n        \n/* critical section */\n\n    \n// release lock\n\n        \n/* remainder section */\n\n\n}\n \nwhile\n \n(\ntrue\n);\n\n\n\n\n\n\n\nSpinlock\n\n\nThe process \"spins\" while waiting for the lock to become available.\n\n\n\n\n5.6 Semaphores\n\u00b6\n\n\nA high-level solution for more complex problems.\n\n\n\n\nA variable \nS\n only accessible by two atomic operations.\n\n\nSpinlock.\n\n\n\n\nwait\n(\nS\n)\n \n{\n               \n/* P */\n\n    \nwhile\n \n(\nS\n \n<=\n \n0\n)\n \n;\n    \n// busy wait\n\n    \nS\n--\n;\n\n\n}\n\n\n\n\n\nsignal\n(\nS\n)\n \n{\n             \n/* V */\n\n    \nS\n++\n;\n\n\n}\n\n\n\n\n\n5.6.1 Semaphore Usage\n\u00b6\n\n\n\n\n\n\nCritical sections:\n\n\ndo\n \n{\n\n    \nwait\n(\nmutex\n);\n\n    \n/* critical section */\n\n    \nsignal\n(\nmutex\n);\n\n    \n/* remainder section */\n\n\n}\n \nwhile\n \n(\ntrue\n);\n\n\n\n\n\n\n\n\n\nPrecedence enforcement:\n\n\n\n\n\n\n$P_1$:\n\n\nS1\n;\n\n\nsignal\n(\nsynch\n);\n\n\n\n\n\n\n\n\n\n$P_2$:\n\n\nwait\n(\nsynch\n);\n\n\nS2\n;\n\n\n\n\n\n\n\n\n\n\n\n\n\n5.6.2 Semaphore Implementation\n\u00b6\n\n\nIt's not good for single CPU.\nEven if it's implemented in a multi-CPU environment, the locks should be held for a short time.\n\n\nWe can implement the Semaphores with block waiting:\n\n\ntypedef\n \nstruct\n \n{\n\n    \nint\n \nvalue\n;\n\n    \nstruct\n \nprocess\n \n*\nlist\n;\n\n\n}\n \nsemaphore\n;\n\n\n\n\n\nwait\n(\nsemaphore\n \n*\nS\n)\n \n{\n\n    \nS\n->\nvalue\n--\n;\n\n    \nif\n \n(\nS\n->\nvalue\n \n<\n \n0\n)\n \n{\n\n        \nadd\n \nthis\n \nprocess\n \nto\n \nS\n->\nlist\n;\n\n        \nblock\n();\n\n    \n}\n\n\n}\n\n\n\n\n\nsignal\n(\nsemaphore\n \n*\nS\n)\n \n{\n\n    \nS\n->\nvalue\n++\n;\n\n    \nif\n \n(\nS\n->\nvalue\n \n<=\n \n0\n)\n \n{\n\n        \nremove\n \na\n \nprocess\n \nP\n \nfrom\n \nS\n->\nlist\n;\n\n        \nwakeup\n(\nP\n);\n\n    \n}\n\n\n}\n\n\n\n\n\n\n\n$|S.value|$ = # of waiting processes if $S.value < 0$.\n\n\n\n\nBounded-waiting can be satisfied by FIFO queue but may be unsatisfied by priority queue.\n\n\n5.6.3 Deadlocks and Starvation\n\u00b6\n\n\n\n\nDeadlock\n\n\nA set of processes is in a deadlock state when every process in the set is waiting for an event that can be caused only by another process in the set.\n\n\n\n\n\\begin{array}{cc}\nP_0 & P_1 \\\\\nwait(S); & wait(Q); \\\\\nwait(Q); & wait(S); \\\\\n\\vdots & \\vdots \\\\\nsignal(S); & signal(Q); \\\\\nsignal(Q); & signal(S); \\\\\n\\end{array}\n\n\n\n\nStarvation (Indefinite blocking)\n\n\nA situation in which processes wait indefinitely within the semaphore.\n\n\ne.g. priority queue, stack (LIFO).\n\n\n\n\n5.6.4 Priority Inversion\n\u00b6\n\n\n\n\nPriority Inversion\n\n\nA higher-priority task is blocked by a lower-priority task due to some resource access conflict.\n\n\n\n\nBinary Semaphore\n\u00b6\n\n\nWe can implement counting semaphores by binary semaphores. ($S_1 = 1$, $S_2 = 0$ and $S_3 = 1$)\n\n\nWAIT\n(\nS\n)\n \n{\n\n    \nwait\n(\nS3\n);\n   \n// protect the whole program\n\n    \nwait\n(\nS1\n);\n   \n// protect C\n\n    \nC\n--\n;\n\n    \nif\n \n(\nC\n \n<\n \n0\n)\n \n{\n\n        \nsignal\n(\nS1\n);\n\n        \nwait\n(\nS2\n);\n\n    \n}\n \nelse\n \nsignal\n(\nS1\n);\n\n    \nsignal\n(\nS3\n);\n\n\n}\n\n\n\n\n\nSIGNAL\n(\nS\n)\n \n{\n\n    \nwait\n(\nS1\n);\n\n    \nC\n++\n;\n\n    \nif\n \n(\nC\n \n<=\n \n0\n)\n\n        \nsignal\n(\nS2\n);\n     \n// wake up\n\n    \nsignal\n(\nS1\n);\n\n\n}\n\n\n\n\n\n\n\nIs \nwait(S3)\n necessary?\n\n\nCan we change the order of \nsignal(S1)\n and \nwait(S2)\n?\n\n\nThere are lots of implementation details.\n\n\n\n\n5.7 Classic Problems of Synchronization\n\u00b6\n\n\n5.7.1 The Bounded-Buffer Problem\n\u00b6\n\n\nint\n \nn\n;\n\n\nsemaphore\n \nmutex\n \n=\n \n1\n;\n\n\nsemaphore\n \nempty\n \n=\n \nn\n;\n\n\nsemaphore\n \nfull\n \n=\n \n0\n;\n\n\n\n\n\ndo\n \n{\n\n    \n/* produce an item in next_produced */\n\n\n    \nwait\n(\nempty\n);\n\n    \nwait\n(\nmutex\n);\n\n\n    \n/* add next_produced to the buffer */\n\n\n    \nsignal\n(\nmutex\n);\n\n    \nsignal\n(\nfull\n);\n\n\n}\n \nwhile\n \n(\ntrue\n);\n\n\n\n\n\ndo\n \n{\n\n    \nwait\n(\nfull\n);\n\n    \nwait\n(\nmutex\n);\n\n\n    \n/* remove an item from buffer to next_consumed */\n\n\n    \nsignal\n(\nmutex\n);\n\n    \nsignal\n(\nempty\n);\n\n\n    \n/* consume the item in next_consumed */\n\n\n}\n \nwhile\n \n(\ntrue\n);\n\n\n\n\n\n5.7.2 The Readers\u2013Writers Problem\n\u00b6\n\n\n\n\n\n\nThe basic assumption:\n\n\n\n\nReaders: shared locks\n\n\nWriters: exclusive locks\n\n\n\n\n\n\n\n\nThe first reader-writers problem\n\n\n\n\nNo readers will be kept waiting unless a writer has already obtained permission to use the shared object $\\to$ potential hazard to writers!\n\n\n\n\n\n\n\n\nThe second reader-writers problem\n\n\n\n\nOne a writer is ready, it performs its write asap $\\to$ potential hazard to readers.\n\n\n\n\n\n\n\n\nsemaphore\n \nrw_mutex\n \n=\n \n1\n;\n\n\nsemaphore\n \nmutex\n \n=\n \n1\n;\n\n\nint\n \nread_count\n \n=\n \n0\n;\n\n\n\n\n\ndo\n \n{\n\n    \nwait\n(\nrw_mutex\n);\n\n\n    \n/* writing is performed */\n\n\n    \nsignal\n(\nrw_mutex\n);\n\n\n}\n \nwhile\n \n(\ntrue\n);\n\n\n\n\n\ndo\n \n{\n\n    \nwait\n(\nmutex\n);\n        \n// protect read_count\n\n    \nread_count\n++\n;\n\n    \nif\n \n(\nread_count\n \n==\n \n1\n)\n\n        \nwait\n(\nrw_mutex\n);\n\n    \nsignal\n(\nmutex\n);\n\n\n    \n/* reading is performed */\n\n\n    \nwait\n(\nmutex\n);\n        \n// protect read_count\n\n    \nread_count\n--\n;\n\n    \nif\n \n(\nread_count\n \n==\n \n0\n)\n\n        \nsignal\n(\nrw_mutex\n);\n\n    \nsignal\n(\nmutex\n);\n\n\n}\n \nwhile\n \n(\ntrue\n);\n\n\n\n\n\n5.7.3 The Dining-Philosophers Problem\n\u00b6\n\n\n\n\nEach philosopher must pick up one chopstick beside him/her at a time.\n\n\nWhen two chopsticks are picked up, the philosopher can eat.\n\n\n\n\nsemaphore\n \nchopstick\n[\n5\n];\n\n\n\n\n\ndo\n \n{\n\n    \nwait\n(\nchopstick\n[\ni\n]);\n\n    \nwait\n(\nchopstick\n[(\ni\n \n+\n \n1\n)\n \n%\n \n5\n]);\n\n\n    \n/* eat for awhile */\n\n\n    \nsignal\n(\nchopstick\n[\ni\n]);\n\n    \nsignal\n(\nchopstick\n[(\ni\n \n+\n \n1\n)\n \n%\n \n5\n]);\n\n\n    \n/* think for awhile */\n\n\n}\n \nwhile\n \n(\ntrue\n);\n\n\n\n\n\nCritical Regions\n\u00b6\n\n\n\n\nRegion $v$ when $C$ (condition) do $S$ (statements)\n\n\nVariable $v$ \u2014 shared among processes and only accessible in the region.\n\n\n\n\n\n\n\n\nstruct\n \nbuffer\n \n{\n\n    \nitem\n \npool\n[\nn\n];\n\n    \nint\n \ncount\n,\n \nin\n,\n \nout\n;\n\n\n};\n\n\n\n\n\n    \nregion\n \nbuffer\n \nwhen\n\n    \n(\ncount\n \n<\n \nn\n)\n \n{\n\n        \npool\n[\nin\n]\n \n=\n \nnext_produced\n;\n\n        \nin\n \n=\n \n(\nin\n \n+\n \n1\n)\n \n%\n \nn\n;\n\n        \ncount\n++\n;\n\n    \n}\n\n\n\n\n\n    \nregion\n \nbuffer\n \nwhen\n\n    \n(\ncount\n \n>\n \n0\n)\n \n{\n\n        \nnext_consumed\n \n=\n \npool\n[\nout\n];\n\n        \nout\n \n=\n \n(\nout\n \n+\n \n1\n)\n \n%\n \nn\n;\n\n        \ncount\n--\n;\n\n    \n}\n\n\n\n\n\n5.8 Monitors\n\u00b6\n\n\n5.8.1 Monitor Usage\n\u00b6",
            "title": "Chapter 5 Process Synchronization"
        },
        {
            "location": "/Chap05/#chapter-5-process-synchronization",
            "text": "",
            "title": "Chapter 5 Process Synchronization"
        },
        {
            "location": "/Chap05/#51-background",
            "text": "Recall  producer\u2013consumer problem . We modify it as follows:  while   ( true )   { \n     /* produce an item in next_produced */ \n\n     while   ( counter   ==   BUFFER_SIZE )   ;      // do nothing \n\n     buffer [ in ]   =   next_produced ; \n     in   =   ( in   +   1 )   %   BUFFER_SIZE ; \n     counter ++ ;  }   while   ( true )   { \n     while   ( counter   ==   0 )   ;        // do nothing \n\n     next_consumed   =   buffer [ out ]; \n     out   =   ( out   +   1 )   %   BUFFER_SIZE ; \n     counter -- ; \n\n     /* consume the item in next_consumed */  }   Suppose  counter == 5  initially. After executing  counter++  in producer or  counter--  in consumer. The value of  counter  may be $4$, $5$, or $6$!  \\begin{array}{lllll}\nT_0: producer & \\text{execute} & register_1 = \\text{counter} & \\{register_1 = 5\\} \\\\\nT_1: producer & \\text{execute} & register_1 = register_1 + 1 & \\{register_1 = 6\\} \\\\\nT_2: consumer & \\text{execute} & register_2 = \\text{counter} & \\{register_2 = 5\\} \\\\\nT_3: consumer & \\text{execute} & register_2 = register_2 - 1 & \\{register_2 = 4\\} \\\\\nT_4: producer & \\text{execute} & \\text{counter} = register_1 & \\{counter = 6\\} \\\\\nT_5: consumer & \\text{execute} & \\text{counter} = register_2 & \\{counter = 4\\}\n\\end{array}   Race condition  Several processes access and manipulate the same data concurrently and the outcome of the execution depends on the  particular order  in which the access takes place.   We need to ensure that only one process at a time can be manipulating the variable  counter .",
            "title": "5.1 Background"
        },
        {
            "location": "/Chap05/#52-the-critical-section-problem",
            "text": "Critical section  In which the process may be changing common variables, updating a table, writing a file, and so on.   do   { \n     /* entry section */ \n         /* critical section */ \n     /* exit section */ \n         /* remainder section */  }   while   ( true );   A solution to the critical-section problem must satisfy:   Mutual exclusion  Progree . Cannot be postponed indefinitely.  Bounded waiting   Two general approches are used to handle critical sections:   Preemptive kernels  Nonpreemptive kernels   Why anyone favor a preemptive kernel over a nonpreemptive one?   Responsive.  More suitable for real-time programming.",
            "title": "5.2 The Critical-Section Problem"
        },
        {
            "location": "/Chap05/#53-petersons-solution",
            "text": "Peterson's solution is restricted to two processes that alternate execution between their critical sections and remainder sections. The processes are named $P_i$ and $P_j$.  Shared data items:  int   turn ;  boolean   flag [ 2 ];   do   { \n     flag [ i ]   =   true ; \n     turn   =   j ; \n     while   ( flag [ j ]   &&   turn   ==   j )   ; \n     /* critical section */ \n\n     flag [ i ]   =   false ; \n     /* remainder section */  }   while   ( true );    If both processes try to enter at the same time,  turn  will be set to both $i$ and $j$ at roughly the same time.  Only one of these assignments will last.   Proof   Mutual exclusion is preserved.  The progress requirement is satisfied.   The bounded-waiting requirement is met.  Suppose $P_i$ execute $turn = j$ first, then $P_j$ execute $turn = i$. In this assumption, $P_i$ will enter its critical section first and $P_j$ will be stocked in the  while(flag[i] && turn == i  (remember that here  i  should be thinked as  j  in the code!). After $P_i$ entering exit section, there are two possibilities:   $P_i$ sets  flag[i] = false , then $P_j$ enters its critical section.  After $P_i$ setting  flag[i] = false , it immediately sets  flag[i] = true  again, consequently, it'll set  turn = j , thus $P_j$ still can enter its critical section.",
            "title": "5.3 Peterson's Solution"
        },
        {
            "location": "/Chap05/#bakery-algorithm",
            "text": "Originally designed for distrubuted systems  Processes which are ready to enter their critical section must take a number and wait till the number becomes the lowest.   int   number [ i ];            // Pi's number if it is nonzeros  boolean   choosing [ i ];      // Pi is taking a number   do   { \n     choosing [ i ]   =   true ;           // A process want to enter its critical section \n     number [ i ]   =   max ( number [ 0 ],   ...,   number [ n   -   1 ])   +   1 ; \n     choosing [ j ]   =   false ;          // A process has got its number \n     for   ( j   =   0 ;   j   <   n ;   j ++ )   { \n         while   ( choosing [ j ])   ; \n         while   ( number [ j ]   !=   0   &&   ( number [ j ],   j )   <   ( number [ i ],   i ))   ;       // If two processes got the same number, then we should compare their indices \n     } \n     /* critical section */ \n\n     number [ i ]   =   0 ; \n     /* remainder section */  }   white   ( true );    An observation: If $P_i$ is in its critical section, and $P_k (k != i)$ , then $(number[i], i) < (number[k], k)$.   Proof   Mutual exclusion: Only the process holds the lowest number can enter the critical section. For each process, when that process doens't get its number, the original process will be stocked in the first while-loop. After that process getting its number, we still need to compare their $numbers$ and $indices$.  Progress requirement: The processes won't be forever postponed.  Bounded-waiting: Assume that a process holds the biggest number, it should wait other processes in the second while-loop. But after all other process entering their exit section and again entering their entry section, they'll get a bigger number, thus the process won't wait forever.",
            "title": "Bakery Algorithm"
        },
        {
            "location": "/Chap05/#54-synchronization-hardware",
            "text": "Disaple interrupt $\\to$ No preemption:   Infeasible in multiprocessor environment.  Potential impacts on interrupt-driven system clocks.    Atomic  Modern computer allow us either to test and modify the content of a word or to swap the contents of two words  atomically \u2014that is, as one uninterruptible.   Although following algorithms satisfy the mutual-exclusion requirement, they don't satisfy the bounded-waiting requirement.  boolean   test_and_set ( boolean   * target )   { \n     boolean   rv   =   * target ; \n     * target   =   true ; \n\n     return   rv ;  }   do   { \n     while   ( test_and_set ( & lock ))   ; \n     /* critical section */ \n\n     lock   =   false ; \n     /* remainder section */  }   while   ( true );   The first process executing  while (test_and_set(&lock))  will set the address value of  lock  to  true  and get the return value  rv = false , thus it won't be stocked in the while-loop and it can enter its critical section.   Mutual exclusion: OK  Progress requirement: OK   Bounded-waiting: FAIL  Assume there is only one CPU, after $P_i$ entering its critical section, $P_j$ will be stocked in the while-loop. After $P_i$ exiting its critical section, there are two possibilities:   $P_i$ sets  lock = false , the CPU context switch to $P_j$, thus $P_j$ can enters its critical section.  After $P_i$ setting  lock = false , the CPU still executes the code of $P_i$, thus $P_i$ enters its critical section again, so $P_j$ may wait forever.     void   swap ( boolean   * a ,   boolean   * b )   { \n     boolean   temp   =   * a ; \n     * a   =   * b ; \n     * b   =   temp ;  }   do   { \n     key   =   true ; \n     while   ( key   ==   true ) \n         swap ( & lock ,   & key ); \n     /* critical section */ \n\n     lock   =   false ; \n     /* remainder section */  }   while   ( true );    Mutual exclusion: OK  Progress requirement: OK  Bounded-waiting: FAIL (the reason is like above)   int   compare_and_swap ( int   * value ,   int   expected ,   int   new_value )   { \n     int   temp   =   * value ; \n\n     if   ( * value   ==   expected ) \n         * value   =   new_value ; \n\n     return   temp ;  }   do   { \n     while   ( compare_and_swap ( & lock ,   0 ,   1 )   !=   0 )   ; \n     /* critical section */ \n\n     lock   =   0 ; \n     /* remainder section */  }   while   ( true );   Following algorithms satisfies all the critical-section requirements.  boolean   waiting [ n ];  boolean   lock ;   do   { \n     waiting [ i ]   =   true ; \n     key   =   true ; \n     while   ( waiting [ i ]   &&   key ) \n         key   =   test_and_set ( & lock ); \n     waiting [ i ]   =   false ; \n     /* critical section */ \n\n     j   =   ( i   +   1 )   %   n ;                      // Assign its next process \n     while   (( j   !=   i )   &&   ! waiting [ j ])       // Find a following process who is waiting \n         j   =   ( j   +   1 )   %   n ; \n\n     if   ( j   ==   i )                           // If no process is waiting \n         lock   =   false ; \n     else \n         waiting [ j ]   =   false ;               // Thus line 4 will be false and Pj won't be stocked anymore \n     /* remainder section */  }   while   ( true );   Assume  lock  is initialized to  false .   Mutual exclusion: If many processes set their  waiting[i] = true , after the first process execute  key = test_and_set(&lock) ,  key  will be set to  false  and  lock  will be set to  true . Therefore, other processes will be stocked in  while (waiting[i] && key)  since their  key  will be set to  true  after  test_and_set(&lock)  ( lock  is now  true ).  Progress requirement: Only the process first run  test_and_set  can enter its critical section.  Bounded-waiting: Wait at most $n - 1$ times.",
            "title": "5.4 Synchronization Hardware"
        },
        {
            "location": "/Chap05/#mutex-locks",
            "text": "A high-level software solution to provide protect critical sections with mutual exclusion.   Acomic execution of  acquire()  and  release() .  Spinlock:  pros: No context switch for multiprocessor systems.  cons: Busy waiting.     acquire ()   { \n     while   ( ! available )   ;          // busy wait \n     available   =   false ;  }   release ()   { \n     available   =   true ;  }   do   { \n     // acquire lock \n         /* critical section */ \n     // release lock \n         /* remainder section */  }   while   ( true );    Spinlock  The process \"spins\" while waiting for the lock to become available.",
            "title": "Mutex Locks"
        },
        {
            "location": "/Chap05/#56-semaphores",
            "text": "A high-level solution for more complex problems.   A variable  S  only accessible by two atomic operations.  Spinlock.   wait ( S )   {                 /* P */ \n     while   ( S   <=   0 )   ;      // busy wait \n     S -- ;  }   signal ( S )   {               /* V */ \n     S ++ ;  }",
            "title": "5.6 Semaphores"
        },
        {
            "location": "/Chap05/#561-semaphore-usage",
            "text": "Critical sections:  do   { \n     wait ( mutex ); \n     /* critical section */ \n     signal ( mutex ); \n     /* remainder section */  }   while   ( true );     Precedence enforcement:    $P_1$:  S1 ;  signal ( synch );     $P_2$:  wait ( synch );  S2 ;",
            "title": "5.6.1 Semaphore Usage"
        },
        {
            "location": "/Chap05/#562-semaphore-implementation",
            "text": "It's not good for single CPU.\nEven if it's implemented in a multi-CPU environment, the locks should be held for a short time.  We can implement the Semaphores with block waiting:  typedef   struct   { \n     int   value ; \n     struct   process   * list ;  }   semaphore ;   wait ( semaphore   * S )   { \n     S -> value -- ; \n     if   ( S -> value   <   0 )   { \n         add   this   process   to   S -> list ; \n         block (); \n     }  }   signal ( semaphore   * S )   { \n     S -> value ++ ; \n     if   ( S -> value   <=   0 )   { \n         remove   a   process   P   from   S -> list ; \n         wakeup ( P ); \n     }  }    $|S.value|$ = # of waiting processes if $S.value < 0$.   Bounded-waiting can be satisfied by FIFO queue but may be unsatisfied by priority queue.",
            "title": "5.6.2 Semaphore Implementation"
        },
        {
            "location": "/Chap05/#563-deadlocks-and-starvation",
            "text": "Deadlock  A set of processes is in a deadlock state when every process in the set is waiting for an event that can be caused only by another process in the set.   \\begin{array}{cc}\nP_0 & P_1 \\\\\nwait(S); & wait(Q); \\\\\nwait(Q); & wait(S); \\\\\n\\vdots & \\vdots \\\\\nsignal(S); & signal(Q); \\\\\nsignal(Q); & signal(S); \\\\\n\\end{array}   Starvation (Indefinite blocking)  A situation in which processes wait indefinitely within the semaphore.  e.g. priority queue, stack (LIFO).",
            "title": "5.6.3 Deadlocks and Starvation"
        },
        {
            "location": "/Chap05/#564-priority-inversion",
            "text": "Priority Inversion  A higher-priority task is blocked by a lower-priority task due to some resource access conflict.",
            "title": "5.6.4 Priority Inversion"
        },
        {
            "location": "/Chap05/#binary-semaphore",
            "text": "We can implement counting semaphores by binary semaphores. ($S_1 = 1$, $S_2 = 0$ and $S_3 = 1$)  WAIT ( S )   { \n     wait ( S3 );     // protect the whole program \n     wait ( S1 );     // protect C \n     C -- ; \n     if   ( C   <   0 )   { \n         signal ( S1 ); \n         wait ( S2 ); \n     }   else   signal ( S1 ); \n     signal ( S3 );  }   SIGNAL ( S )   { \n     wait ( S1 ); \n     C ++ ; \n     if   ( C   <=   0 ) \n         signal ( S2 );       // wake up \n     signal ( S1 );  }    Is  wait(S3)  necessary?  Can we change the order of  signal(S1)  and  wait(S2) ?  There are lots of implementation details.",
            "title": "Binary Semaphore"
        },
        {
            "location": "/Chap05/#57-classic-problems-of-synchronization",
            "text": "",
            "title": "5.7 Classic Problems of Synchronization"
        },
        {
            "location": "/Chap05/#571-the-bounded-buffer-problem",
            "text": "int   n ;  semaphore   mutex   =   1 ;  semaphore   empty   =   n ;  semaphore   full   =   0 ;   do   { \n     /* produce an item in next_produced */ \n\n     wait ( empty ); \n     wait ( mutex ); \n\n     /* add next_produced to the buffer */ \n\n     signal ( mutex ); \n     signal ( full );  }   while   ( true );   do   { \n     wait ( full ); \n     wait ( mutex ); \n\n     /* remove an item from buffer to next_consumed */ \n\n     signal ( mutex ); \n     signal ( empty ); \n\n     /* consume the item in next_consumed */  }   while   ( true );",
            "title": "5.7.1 The Bounded-Buffer Problem"
        },
        {
            "location": "/Chap05/#572-the-readerswriters-problem",
            "text": "The basic assumption:   Readers: shared locks  Writers: exclusive locks     The first reader-writers problem   No readers will be kept waiting unless a writer has already obtained permission to use the shared object $\\to$ potential hazard to writers!     The second reader-writers problem   One a writer is ready, it performs its write asap $\\to$ potential hazard to readers.     semaphore   rw_mutex   =   1 ;  semaphore   mutex   =   1 ;  int   read_count   =   0 ;   do   { \n     wait ( rw_mutex ); \n\n     /* writing is performed */ \n\n     signal ( rw_mutex );  }   while   ( true );   do   { \n     wait ( mutex );          // protect read_count \n     read_count ++ ; \n     if   ( read_count   ==   1 ) \n         wait ( rw_mutex ); \n     signal ( mutex ); \n\n     /* reading is performed */ \n\n     wait ( mutex );          // protect read_count \n     read_count -- ; \n     if   ( read_count   ==   0 ) \n         signal ( rw_mutex ); \n     signal ( mutex );  }   while   ( true );",
            "title": "5.7.2 The Readers\u2013Writers Problem"
        },
        {
            "location": "/Chap05/#573-the-dining-philosophers-problem",
            "text": "Each philosopher must pick up one chopstick beside him/her at a time.  When two chopsticks are picked up, the philosopher can eat.   semaphore   chopstick [ 5 ];   do   { \n     wait ( chopstick [ i ]); \n     wait ( chopstick [( i   +   1 )   %   5 ]); \n\n     /* eat for awhile */ \n\n     signal ( chopstick [ i ]); \n     signal ( chopstick [( i   +   1 )   %   5 ]); \n\n     /* think for awhile */  }   while   ( true );",
            "title": "5.7.3 The Dining-Philosophers Problem"
        },
        {
            "location": "/Chap05/#critical-regions",
            "text": "Region $v$ when $C$ (condition) do $S$ (statements)  Variable $v$ \u2014 shared among processes and only accessible in the region.     struct   buffer   { \n     item   pool [ n ]; \n     int   count ,   in ,   out ;  };        region   buffer   when \n     ( count   <   n )   { \n         pool [ in ]   =   next_produced ; \n         in   =   ( in   +   1 )   %   n ; \n         count ++ ; \n     }        region   buffer   when \n     ( count   >   0 )   { \n         next_consumed   =   pool [ out ]; \n         out   =   ( out   +   1 )   %   n ; \n         count -- ; \n     }",
            "title": "Critical Regions"
        },
        {
            "location": "/Chap05/#58-monitors",
            "text": "",
            "title": "5.8 Monitors"
        },
        {
            "location": "/Chap05/#581-monitor-usage",
            "text": "",
            "title": "5.8.1 Monitor Usage"
        },
        {
            "location": "/Chap06/",
            "text": "Chapter 6 CPU Scheduling\n\u00b6\n\n\n6.1 Basic Concepts\n\u00b6\n\n\nThe objective of multiprogramming is to have some process running at all times, to maximize CPU utilization.\n\n\nA process is executed until it must wait, typically for the completion of some I/O request.\n\n\n6.1.1 CPU\u2013I/O Burst Cycle\n\u00b6\n\n\nProcess execution = cycle of CPU + I/O wait.\n\n\n\n\n6.1.2 CPU Scheduler\n\u00b6\n\n\nShort-term scheduler\n (CPU scheduler): select process in the ready queue.\n\n\n6.1.3 Preemptive Scheduling\n\u00b6\n\n\nCPU-scheduling decisions when a process:\n\n\n\n\n(nonpreemptive) Switches from the running state $\\to$ the waiting state (e.g. I/O request, \nwait()\n for child)\n\n\n(preemptive) Switches from the running state $\\to$ the ready state (e.g. interrupt)\n\n\n(preemptive) Switches from the waiting state $\\to$ the ready state (e.g. completion of I/O)\n\n\n(nonpreemptive) Terminates\n\n\n\n\n6.1.4 Dispatcher\n\u00b6\n\n\n\n\nDispatcher\n\n\nThe module that gives control of the CPU to \nthe process selected by the short-term scheduler\n. It should be fast.\n\n\n\n\n\n\nSwitching context\n\n\nSwitching to user mode\n\n\nJumping to the proper location in the user program to restart that program\n\n\n\n\n\n\nDispatch latency\n\n\nThe time it takes for the dispatcher to stop one process and start another running.\n\n\nStop a process $\\leftrightarrow$ Start a process\n\n\n\n\n6.2 Scheduling Criteria\n\u00b6\n\n\n\n\nCPU utilization\n\n\nThroughput\n\n\nTurnaround time\n: $\\text{Completion Time} - \\text{Start Time}$.\n\n\nWaiting time\n: The sum of the periods spent waiting in the ready queue.\n\n\nResponse time\n\n\n\n\n6.3 Scheduling Algorithms\n\u00b6\n\n\n6.3-1 First-Come, First-Served Scheduling\n\u00b6\n\n\n\n\nThe process which requests the CPU first is allocated the CPU.\n\n\n\n\nProperties:\n\n\n\n\nNonpreemptive FCFS.\n\n\nCPU might be hold for an extended period.\n\n\n\n\n\n\n\n\nCritical problem: Convoy effect!\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\nGiven processes:\n\n\n\n\n\n\n\n\nProcess\n\n\nBusrt Time\n\n\n\n\n\n\n\n\n\n\n$P_1$\n\n\n24\n\n\n\n\n\n\n$P_2$\n\n\n3\n\n\n\n\n\n\n$P_3$\n\n\n3\n\n\n\n\n\n\n\n\n\n\n\n\nConsider order: $P_1 \\to P_2 \\to P_3$:\n\n\n\n\n\n\nGantt chart:\n\n\n\n\n\n\n\n\nAverage waiting time = (0 + 24 + 27) / 3 = 17 ms.\n\n\n\n\n\n\n\n\n\n\nConsider order: $P_2 \\to P_3 \\to P_1$:\n\n\n\n\n\n\nGantt chart:\n\n\n\n\n\n\n\n\nAverage waiting time = (0 + 3 + 6) / 3 = 9 ms.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConvoy effect\nAll the other processes wait for the one big process to get off the CPU.\n\n\n\n\n\n\n6.3.2 Shortest-Job-First Scheduling\n\u00b6\n\n\n\n\n\n\nProperties:\n\n\n\n\nNonpreemptive SJF\n\n\nShortest-next-CPU-burst first\n\n\n\n\n\n\n\n\nProblem: Measure the future!\n\n\n\n\n\n\nExample 1:\n\n\n\n\n\n\nGiven processes:\n\n\n\n\n\n\n\n\nProcess\n\n\nBurst Time\n\n\n\n\n\n\n\n\n\n\n$P_1$\n\n\n6\n\n\n\n\n\n\n$P_2$\n\n\n8\n\n\n\n\n\n\n$P_3$\n\n\n7\n\n\n\n\n\n\n$P_4$\n\n\n3\n\n\n\n\n\n\n\n\n\n\n\n\nBy SJF scheduling:\n\n\n\n\n\n\nGantt chart:\n\n\n\n\n\n\n\n\nAverage waiting time = (3 + 16 + 9 + 0) / 4 = 7 ms.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSJF is used frequently in long-term (job) scheduling, but it cannot be implemented at the level of short-term CPU scheduling.\n\n\n\n\n\n\nExponential average\n\n\nLet $t_n$ be time of $n$th CPU burst, and $\\tau_{n + 1}$ be the next CPU burst.\n\n\n\\begin{align}\n\\tau_{n + 1} & = \\alpha t_n + (1 - \\alpha)\\tau_n, \\quad 0 \\le \\alpha \\le 1. \\\\\n             & = \\alpha t_n + (1 - \\alpha)\\alpha t_{n - 1} + \\cdots + (1 - \\alpha)^j \\alpha t_{n - j} + \\cdots + (1 - \\alpha)^{n + 1}\\tau_0.\n\\end{align}\n\n\n\n\n\n\n\n\nExample 2:\n\n\n\n\n\n\nGiven processes:\n\n\n\n\n\n\n\n\nProcess\n\n\nArrival Time\n\n\nBurst Time\n\n\n\n\n\n\n\n\n\n\n$P_1$\n\n\n0\n\n\n8\n\n\n\n\n\n\n$P_2$\n\n\n1\n\n\n4\n\n\n\n\n\n\n$P_3$\n\n\n2\n\n\n9\n\n\n\n\n\n\n$P_4$\n\n\n3\n\n\n5\n\n\n\n\n\n\n\n\n\n\n\n\nBy preemptive SJF scheduling:\n\n\n\n\n\n\nGantt chart:\n\n\n\n\n\n\n\n\nAverage waiting time = [(10 - 1) + (1 - 1) + (17 - 2) + (5 - 3)] / 4 = 26 / 4 = 6.5 ms.\n\n\n\n\n\n\n\n\n\n\n6.3.3 Priority Scheduling\n\u00b6\n\n\n\n\n\n\nProperties: \n\n\n\n\nCPU is assigned to the process with the highest priority \u2014 A framework for various scheduling algorithms:\n\n\nFCFS: Equal-priority with tie-breaking\n\n\nSJF: Priority = 1 / next CPU burst length\n\n\n\n\n\n\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\nGiven processes:\n\n\n\n\n\n\n\n\nProcess\n\n\nBurst Time\n\n\nPriority\n\n\n\n\n\n\n\n\n\n\n$P_1$\n\n\n10\n\n\n3\n\n\n\n\n\n\n$P_2$\n\n\n1\n\n\n1\n\n\n\n\n\n\n$P_3$\n\n\n2\n\n\n4\n\n\n\n\n\n\n$P_4$\n\n\n1\n\n\n5\n\n\n\n\n\n\n$P_5$\n\n\n5\n\n\n2\n\n\n\n\n\n\n\n\n\n\n\n\nBy preemptive SJF scheduling:\n\n\n\n\n\n\nGantt chart:\n\n\n\n\n\n\n\n\nAverage waiting time = 8.2 ms. (How?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nProblem with priority scheduling\n\n\n\n\nindefinite blocking: low-priority processes could starve to death!\n\n\nstarvation\n\n\n\n\n\n\n\n\nAging\n\n\nAging gradually increase the priority of processes that wait in the system for a long time.\n\n\n\n\n6.3.4 Round-Robin Scheduling\n\u00b6\n\n\n\n\nRR is similar to FCFS except that preemption is added to switch between processes.\n\n\n\n\nGoal: fairness, time sharing.\n\n\n\n\n\n\nExample:\n\n\n\n\n\n\nGiven processes with quantum = 4ms:\n\n\n\n\n\n\n\n\nProcess\n\n\nBurst Time\n\n\n\n\n\n\n\n\n\n\n$P_1$\n\n\n24\n\n\n\n\n\n\n$P_2$\n\n\n3\n\n\n\n\n\n\n$P_3$\n\n\n3\n\n\n\n\n\n\n\n\n\n\n\n\nGantt chart:\n\n\n\n\n\n\n\n\nAverage waiting time = [(10 - 4) + 4 + 7] / 3 = 5.66 ms.\n\n\n\n\n\n\n\n\n\n\n\n\nAlthough the time quantum should be large compared with the context switch time, it should not be too large.\n\n\n\n\n6.3.5 Multilevel Queue Scheduling\n\u00b6\n\n\n\n\n\n\nIntra-queue scheduling\n\n\n\n\nIndependent choice of scheduling algorithms\n\n\n\n\n\n\n\n\nInter-queue scheduling\n\n\n\n\nFixed-priority preemptive scheduling\n\n\ne.g., foreground queues always have absolute priority over the background queues.\n\n\n\n\n\n\nTime slice between queues\n\n\ne.g., 80% CPU is given to goreground processes, and 20% CPU is given to background processes.\n\n\n\n\n\n\n\n\n\n\n\n\nEach queue has absolute priority over lower-priority queues.\n\n\nIf an interactive editing process entered the ready queue while a batch process was running, the batch process would be preempted.\n\n\n\n\n6.3.6 Multilevel Feedback Queue Scheduling\n\u00b6\n\n\n\n\nMultilevel feedback queue allows a process to move between queues.\n\n\n\n\nA multilevel feedback queue is defined by:\n\n\n\n\n#queues\n\n\nThe scheduling algorithm for each queue\n\n\n\n\nThe method used to determine when to \n\n\n\n\nupgrade a process to a higher priority queue\n\n\ndemote a process to a lower priority queue\n\n\n\n\n\n\n\n\nThe method used to determine which queue a process will enter when that process needs service\n\n\n\n\n\n\n6.4 Thread Scheduling\n\u00b6\n\n\n6.4.1 Contention Scope\n\u00b6\n\n\n\n\nProcess Contention Scope (PCS)\n\n\nOn systems implementing the \nmany-to-one\n and \nmany-to-many\n models, the thread library schedules user-level threads to run on an available LWP, since competition for the CPU takes place among threads belonging to the same process.\n\n\nIt is important to note that PCS will typically preempt the thread currently running in favor of a higher-priority thread\n\n\n\n\n\n\nSystem Contention Scope (SCS)\n\n\nCompetition for the CPU with SCS scheduling takes place among all threads in the system. Systems using the \none-to-one\n model.\n\n\n\n\n6.4.2 Pthread Scheduling\n\u00b6\n\n\n\n\nPCS: \nPTHREAD_SCOPE_PROCESS\n\n\nSCS: \nPTHREAD_SCOPE_SYSTEM\n\n\n\n\n2 methods:\n\n\n\n\npthread_attr_setscope(pthread attr_t *attr, int scope)\n\n\npthread_attr_getscope(pthread attr_t *attr, int *scope)\n\n\n\n\n6.5 Multiple-Processor Scheduling\n\u00b6\n\n\n6.5.1 Approaches to Multiple-Processor Scheduling\n\u00b6\n\n\n\n\nAsymmetric multiprocessing\n: only \nthe master server\n process accesses the system data structures, reducing the need for data sharing.\n\n\nSymmetric multiprocessing (SMP)\n: each processor is self-scheduling.\n\n\n\n\n6.5.2 Processor Affinity\n\u00b6\n\n\n\n\nProcessor Affinity\n\n\nA process has an affinity for the processor on which it is currently running.\n\n\n\n\n\n\nSoft affinity\n\n\nHard affinity\n (e.g. Linux: \nsched_setaffinity()\n)\n\n\n\n\n\n\nNon-Uniform Memory Access (NUMA)\n\n\nA CPU has faster access to some parts of main memory than to other parts.\n\n\n\n\n\n\n6.5.3 Load Balancing\n\u00b6\n\n\nOn systems with a common run queue, load balancing is often unnecessary, because once a processor becomes idle, it immediately extracts a runnable process from the common run queue.\n\n\n\n\nPush migration\n: pushing processes from overloaded to less-busy processors.\n\n\nPull migration\n: pulling a waiting task from a busy processor.\n\n\n\n\n6.5.4 Multicore Processors\n\u00b6\n\n\nSMP systems that use multicore processors are faster and consume less power than systems in which each processor has its own physical chip.\n\n\nThere are two ways to multithread a processing core:\n\n\n\n\nCoarse-grained\n: a thread executes on a processor until a long-latency event such as a memory stall occurs.\n\n\nFind-grained (interleaved)\n: switches between threads at a much finer level of granularity\n\n\n\n\n6.6 Real-Time CPU Scheduling\n\u00b6\n\n\n\n\nSoft real-time systems\n\n\nHard real-time systems\n\n\n\n\n6.6.1 Minimizing Latency\n\u00b6\n\n\n\n\nEvent latency\n\n\nThe amount of time that elapses from when an event occurs to when it is serviced.\n\n\n\n\nThere are 2 types:\n\n\n\n\nInterrupt latency\n\n\nDispatch latency\n\n\n\n\n\n\nInterrupt latency\n\n\nThe period of time from the arrival of an interrupt at the CPU to the start of the interrupt service routine (ISR) that services the interrupt.\n\n\n\n\n\n\nDispatch latency\n\n\nThe amount of time required for the scheduling dispatcher to stop one process and start another.\n\n\n\n\n6.6.2 Priority-Based Scheduling\n\u00b6\n\n\nThe processes are considered \nperiodic\n. That is, they require the CPU at constant intervals (periods). ($t$: fixed processing time, $d$: deadline, and $p$: period.)\n\n\n$$0 \\le t \\le d \\le p.$$\n\n\n$$rate = 1 / p.$$\n\n\nAdmission-control\n:\n\n\n\n\nAdmits the process.\n\n\nRejects the request.\n\n\n\n\n6.6.3 Rate-Monotonic Scheduling\n\u00b6\n\n\n\n\n\n\nExample 1:\n\n\n\n\n\n\nGiven processes: (the deadline for each process requires that it complete its CPU burst by the start of its next period.)\n\n\n\n\n\n\n\n\nProcess\n\n\nPeriod\n\n\nProcTime\n\n\n\n\n\n\n\n\n\n\n$P_1$\n\n\n$p_1 = 50$\n\n\n$t_1 = 20$\n\n\n\n\n\n\n$P_2$\n\n\n$p_2 = 100$\n\n\n$t_2 = 35$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample 2:\n\n\n\n\n\n\nGiven processes:\n\n\n\n\n\n\n\n\nProcess\n\n\nPeriod\n\n\nProcTime\n\n\n\n\n\n\n\n\n\n\n$P_1$\n\n\n$p_1 = 50$\n\n\n$t_1 = 25$\n\n\n\n\n\n\n$P_2$\n\n\n$p_2 = 80$\n\n\n$t_2 = 35$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6.6.4 Earliest-Deadline-First Scheduling\n\u00b6\n\n\n\n\n6.6.5 Proportional Share Scheduling\n\u00b6\n\n\nProportional share schedulers operate by allocating $T$ shares among all applications. An application can receive $N$ shares of time.\n\n\n6.6.6 POSIX Real-Time Scheduling\n\u00b6\n\n\n\n\n\n\nPOSIX.1b \u2014 Extensions for real-time computing\n\n\n\n\nSCHED_FIFO\n\n\nSCHED_RR\n\n\nSCHED_OTHER\n\n\n\n\n\n\n\n\nAPI\n\n\n\n\npthread_attr_getsched_policy(pthread_attr_t *attr, int *policy)\n\n\npthread_attr_setsched_policy(pthread_attr_t *attr, int *policy)\n\n\n\n\n\n\n\n\n6.7 Operating-System Examples\n\u00b6\n\n\n6.7.1 Example: Linux Scheduling\n\u00b6\n\n\n6.7.2 Example: Windows Scheduling\n\u00b6\n\n\n6.7.3 Example: Solaris Scheduling\n\u00b6\n\n\n6.8 Algorithm Evaluation\n\u00b6\n\n\n6.8.1 Deterministic Modeling\n\u00b6\n\n\n6.8.2 Queueing Models\n\u00b6\n\n\n\n\nLittle's formula ($n$: # of processes in the queue, $\\lambda$: arrival rate, $W$: average waiting time in the queue.)\n\n\n\n\n$$n = \\lambda \\times W.$$\n\n\n6.8.3 Simulations\n\u00b6\n\n\n\n\n\n\nProperties:\n\n\n\n\nAccurate but expensive\n\n\n\n\n\n\n\n\nProcedures:\n\n\n\n\nProgram a model of the computer system\n\n\nDrive the simulation with various data sets\n\n\n\n\n\n\n\n\n6.8.4 Implementation\n\u00b6",
            "title": "Chapter 6 CPU Scheduling"
        },
        {
            "location": "/Chap06/#chapter-6-cpu-scheduling",
            "text": "",
            "title": "Chapter 6 CPU Scheduling"
        },
        {
            "location": "/Chap06/#61-basic-concepts",
            "text": "The objective of multiprogramming is to have some process running at all times, to maximize CPU utilization.  A process is executed until it must wait, typically for the completion of some I/O request.",
            "title": "6.1 Basic Concepts"
        },
        {
            "location": "/Chap06/#611-cpuio-burst-cycle",
            "text": "Process execution = cycle of CPU + I/O wait.",
            "title": "6.1.1 CPU\u2013I/O Burst Cycle"
        },
        {
            "location": "/Chap06/#612-cpu-scheduler",
            "text": "Short-term scheduler  (CPU scheduler): select process in the ready queue.",
            "title": "6.1.2 CPU Scheduler"
        },
        {
            "location": "/Chap06/#613-preemptive-scheduling",
            "text": "CPU-scheduling decisions when a process:   (nonpreemptive) Switches from the running state $\\to$ the waiting state (e.g. I/O request,  wait()  for child)  (preemptive) Switches from the running state $\\to$ the ready state (e.g. interrupt)  (preemptive) Switches from the waiting state $\\to$ the ready state (e.g. completion of I/O)  (nonpreemptive) Terminates",
            "title": "6.1.3 Preemptive Scheduling"
        },
        {
            "location": "/Chap06/#614-dispatcher",
            "text": "Dispatcher  The module that gives control of the CPU to  the process selected by the short-term scheduler . It should be fast.    Switching context  Switching to user mode  Jumping to the proper location in the user program to restart that program    Dispatch latency  The time it takes for the dispatcher to stop one process and start another running.  Stop a process $\\leftrightarrow$ Start a process",
            "title": "6.1.4 Dispatcher"
        },
        {
            "location": "/Chap06/#62-scheduling-criteria",
            "text": "CPU utilization  Throughput  Turnaround time : $\\text{Completion Time} - \\text{Start Time}$.  Waiting time : The sum of the periods spent waiting in the ready queue.  Response time",
            "title": "6.2 Scheduling Criteria"
        },
        {
            "location": "/Chap06/#63-scheduling-algorithms",
            "text": "",
            "title": "6.3 Scheduling Algorithms"
        },
        {
            "location": "/Chap06/#63-1-first-come-first-served-scheduling",
            "text": "The process which requests the CPU first is allocated the CPU.   Properties:   Nonpreemptive FCFS.  CPU might be hold for an extended period.     Critical problem: Convoy effect!    Example:    Given processes:     Process  Busrt Time      $P_1$  24    $P_2$  3    $P_3$  3       Consider order: $P_1 \\to P_2 \\to P_3$:    Gantt chart:     Average waiting time = (0 + 24 + 27) / 3 = 17 ms.      Consider order: $P_2 \\to P_3 \\to P_1$:    Gantt chart:     Average waiting time = (0 + 3 + 6) / 3 = 9 ms.         Convoy effect All the other processes wait for the one big process to get off the CPU.",
            "title": "6.3-1 First-Come, First-Served Scheduling"
        },
        {
            "location": "/Chap06/#632-shortest-job-first-scheduling",
            "text": "Properties:   Nonpreemptive SJF  Shortest-next-CPU-burst first     Problem: Measure the future!    Example 1:    Given processes:     Process  Burst Time      $P_1$  6    $P_2$  8    $P_3$  7    $P_4$  3       By SJF scheduling:    Gantt chart:     Average waiting time = (3 + 16 + 9 + 0) / 4 = 7 ms.         SJF is used frequently in long-term (job) scheduling, but it cannot be implemented at the level of short-term CPU scheduling.    Exponential average  Let $t_n$ be time of $n$th CPU burst, and $\\tau_{n + 1}$ be the next CPU burst.  \\begin{align}\n\\tau_{n + 1} & = \\alpha t_n + (1 - \\alpha)\\tau_n, \\quad 0 \\le \\alpha \\le 1. \\\\\n             & = \\alpha t_n + (1 - \\alpha)\\alpha t_{n - 1} + \\cdots + (1 - \\alpha)^j \\alpha t_{n - j} + \\cdots + (1 - \\alpha)^{n + 1}\\tau_0.\n\\end{align}     Example 2:    Given processes:     Process  Arrival Time  Burst Time      $P_1$  0  8    $P_2$  1  4    $P_3$  2  9    $P_4$  3  5       By preemptive SJF scheduling:    Gantt chart:     Average waiting time = [(10 - 1) + (1 - 1) + (17 - 2) + (5 - 3)] / 4 = 26 / 4 = 6.5 ms.",
            "title": "6.3.2 Shortest-Job-First Scheduling"
        },
        {
            "location": "/Chap06/#633-priority-scheduling",
            "text": "Properties:    CPU is assigned to the process with the highest priority \u2014 A framework for various scheduling algorithms:  FCFS: Equal-priority with tie-breaking  SJF: Priority = 1 / next CPU burst length       Example:    Given processes:     Process  Burst Time  Priority      $P_1$  10  3    $P_2$  1  1    $P_3$  2  4    $P_4$  1  5    $P_5$  5  2       By preemptive SJF scheduling:    Gantt chart:     Average waiting time = 8.2 ms. (How?)         Problem with priority scheduling   indefinite blocking: low-priority processes could starve to death!  starvation     Aging  Aging gradually increase the priority of processes that wait in the system for a long time.",
            "title": "6.3.3 Priority Scheduling"
        },
        {
            "location": "/Chap06/#634-round-robin-scheduling",
            "text": "RR is similar to FCFS except that preemption is added to switch between processes.   Goal: fairness, time sharing.    Example:    Given processes with quantum = 4ms:     Process  Burst Time      $P_1$  24    $P_2$  3    $P_3$  3       Gantt chart:     Average waiting time = [(10 - 4) + 4 + 7] / 3 = 5.66 ms.       Although the time quantum should be large compared with the context switch time, it should not be too large.",
            "title": "6.3.4 Round-Robin Scheduling"
        },
        {
            "location": "/Chap06/#635-multilevel-queue-scheduling",
            "text": "Intra-queue scheduling   Independent choice of scheduling algorithms     Inter-queue scheduling   Fixed-priority preemptive scheduling  e.g., foreground queues always have absolute priority over the background queues.    Time slice between queues  e.g., 80% CPU is given to goreground processes, and 20% CPU is given to background processes.       Each queue has absolute priority over lower-priority queues.  If an interactive editing process entered the ready queue while a batch process was running, the batch process would be preempted.",
            "title": "6.3.5 Multilevel Queue Scheduling"
        },
        {
            "location": "/Chap06/#636-multilevel-feedback-queue-scheduling",
            "text": "Multilevel feedback queue allows a process to move between queues.   A multilevel feedback queue is defined by:   #queues  The scheduling algorithm for each queue   The method used to determine when to    upgrade a process to a higher priority queue  demote a process to a lower priority queue     The method used to determine which queue a process will enter when that process needs service",
            "title": "6.3.6 Multilevel Feedback Queue Scheduling"
        },
        {
            "location": "/Chap06/#64-thread-scheduling",
            "text": "",
            "title": "6.4 Thread Scheduling"
        },
        {
            "location": "/Chap06/#641-contention-scope",
            "text": "Process Contention Scope (PCS)  On systems implementing the  many-to-one  and  many-to-many  models, the thread library schedules user-level threads to run on an available LWP, since competition for the CPU takes place among threads belonging to the same process.  It is important to note that PCS will typically preempt the thread currently running in favor of a higher-priority thread    System Contention Scope (SCS)  Competition for the CPU with SCS scheduling takes place among all threads in the system. Systems using the  one-to-one  model.",
            "title": "6.4.1 Contention Scope"
        },
        {
            "location": "/Chap06/#642-pthread-scheduling",
            "text": "PCS:  PTHREAD_SCOPE_PROCESS  SCS:  PTHREAD_SCOPE_SYSTEM   2 methods:   pthread_attr_setscope(pthread attr_t *attr, int scope)  pthread_attr_getscope(pthread attr_t *attr, int *scope)",
            "title": "6.4.2 Pthread Scheduling"
        },
        {
            "location": "/Chap06/#65-multiple-processor-scheduling",
            "text": "",
            "title": "6.5 Multiple-Processor Scheduling"
        },
        {
            "location": "/Chap06/#651-approaches-to-multiple-processor-scheduling",
            "text": "Asymmetric multiprocessing : only  the master server  process accesses the system data structures, reducing the need for data sharing.  Symmetric multiprocessing (SMP) : each processor is self-scheduling.",
            "title": "6.5.1 Approaches to Multiple-Processor Scheduling"
        },
        {
            "location": "/Chap06/#652-processor-affinity",
            "text": "Processor Affinity  A process has an affinity for the processor on which it is currently running.    Soft affinity  Hard affinity  (e.g. Linux:  sched_setaffinity() )    Non-Uniform Memory Access (NUMA)  A CPU has faster access to some parts of main memory than to other parts.",
            "title": "6.5.2 Processor Affinity"
        },
        {
            "location": "/Chap06/#653-load-balancing",
            "text": "On systems with a common run queue, load balancing is often unnecessary, because once a processor becomes idle, it immediately extracts a runnable process from the common run queue.   Push migration : pushing processes from overloaded to less-busy processors.  Pull migration : pulling a waiting task from a busy processor.",
            "title": "6.5.3 Load Balancing"
        },
        {
            "location": "/Chap06/#654-multicore-processors",
            "text": "SMP systems that use multicore processors are faster and consume less power than systems in which each processor has its own physical chip.  There are two ways to multithread a processing core:   Coarse-grained : a thread executes on a processor until a long-latency event such as a memory stall occurs.  Find-grained (interleaved) : switches between threads at a much finer level of granularity",
            "title": "6.5.4 Multicore Processors"
        },
        {
            "location": "/Chap06/#66-real-time-cpu-scheduling",
            "text": "Soft real-time systems  Hard real-time systems",
            "title": "6.6 Real-Time CPU Scheduling"
        },
        {
            "location": "/Chap06/#661-minimizing-latency",
            "text": "Event latency  The amount of time that elapses from when an event occurs to when it is serviced.   There are 2 types:   Interrupt latency  Dispatch latency    Interrupt latency  The period of time from the arrival of an interrupt at the CPU to the start of the interrupt service routine (ISR) that services the interrupt.    Dispatch latency  The amount of time required for the scheduling dispatcher to stop one process and start another.",
            "title": "6.6.1 Minimizing Latency"
        },
        {
            "location": "/Chap06/#662-priority-based-scheduling",
            "text": "The processes are considered  periodic . That is, they require the CPU at constant intervals (periods). ($t$: fixed processing time, $d$: deadline, and $p$: period.)  $$0 \\le t \\le d \\le p.$$  $$rate = 1 / p.$$  Admission-control :   Admits the process.  Rejects the request.",
            "title": "6.6.2 Priority-Based Scheduling"
        },
        {
            "location": "/Chap06/#663-rate-monotonic-scheduling",
            "text": "Example 1:    Given processes: (the deadline for each process requires that it complete its CPU burst by the start of its next period.)     Process  Period  ProcTime      $P_1$  $p_1 = 50$  $t_1 = 20$    $P_2$  $p_2 = 100$  $t_2 = 35$          Example 2:    Given processes:     Process  Period  ProcTime      $P_1$  $p_1 = 50$  $t_1 = 25$    $P_2$  $p_2 = 80$  $t_2 = 35$",
            "title": "6.6.3 Rate-Monotonic Scheduling"
        },
        {
            "location": "/Chap06/#664-earliest-deadline-first-scheduling",
            "text": "",
            "title": "6.6.4 Earliest-Deadline-First Scheduling"
        },
        {
            "location": "/Chap06/#665-proportional-share-scheduling",
            "text": "Proportional share schedulers operate by allocating $T$ shares among all applications. An application can receive $N$ shares of time.",
            "title": "6.6.5 Proportional Share Scheduling"
        },
        {
            "location": "/Chap06/#666-posix-real-time-scheduling",
            "text": "POSIX.1b \u2014 Extensions for real-time computing   SCHED_FIFO  SCHED_RR  SCHED_OTHER     API   pthread_attr_getsched_policy(pthread_attr_t *attr, int *policy)  pthread_attr_setsched_policy(pthread_attr_t *attr, int *policy)",
            "title": "6.6.6 POSIX Real-Time Scheduling"
        },
        {
            "location": "/Chap06/#67-operating-system-examples",
            "text": "",
            "title": "6.7 Operating-System Examples"
        },
        {
            "location": "/Chap06/#671-example-linux-scheduling",
            "text": "",
            "title": "6.7.1 Example: Linux Scheduling"
        },
        {
            "location": "/Chap06/#672-example-windows-scheduling",
            "text": "",
            "title": "6.7.2 Example: Windows Scheduling"
        },
        {
            "location": "/Chap06/#673-example-solaris-scheduling",
            "text": "",
            "title": "6.7.3 Example: Solaris Scheduling"
        },
        {
            "location": "/Chap06/#68-algorithm-evaluation",
            "text": "",
            "title": "6.8 Algorithm Evaluation"
        },
        {
            "location": "/Chap06/#681-deterministic-modeling",
            "text": "",
            "title": "6.8.1 Deterministic Modeling"
        },
        {
            "location": "/Chap06/#682-queueing-models",
            "text": "Little's formula ($n$: # of processes in the queue, $\\lambda$: arrival rate, $W$: average waiting time in the queue.)   $$n = \\lambda \\times W.$$",
            "title": "6.8.2 Queueing Models"
        },
        {
            "location": "/Chap06/#683-simulations",
            "text": "Properties:   Accurate but expensive     Procedures:   Program a model of the computer system  Drive the simulation with various data sets",
            "title": "6.8.3 Simulations"
        },
        {
            "location": "/Chap06/#684-implementation",
            "text": "",
            "title": "6.8.4 Implementation"
        }
    ]
}