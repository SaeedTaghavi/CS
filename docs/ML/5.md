# Lecture 5: Logistic Regression

我們想找的是：$P_{w, b} (C_1 \mid x)$

$$
\begin{cases}
P_{w, b} (C_1 \mid x) \ge 0.5 & \text{output: } C_1, \\\\
\text{else}                   & \text{output: } C_2
\end{cases}
$$

\begin{align}
P_{w, b} (C_1 \mid x)
  & = \sigma(z) \\\\
  & = \sigma(w \cdot x + b)
\end{align}

我們會有以下的 Function set（包含各種不同的 $w$ 和 $b$）:

$$f_{w, b}(x) = P_{w, b} (C_1 \mid x)$$

![](https://i.imgur.com/QlMME4G.png)

下面對 Logistic Regression 和 Linear Regression 做比較：

| | Logistic Regression | Linear Regression |
|:--:|:--:|:--:|
| $f_{w, b}(x)$ | $\sigma (\sum_i w_ix_i + b)$ | $\sum_i w_ix_i + b$ | 
| Output | between $0$ and $1$ | any value |
| Training data | $(x^n, \hat y^n)$ | $(x^n, \hat y^n)$ |
| $\hat y^n$ | $1$ for class 1, $0$ for class 2 | a real number |
| $L(f)$ |$\sum_n C(f(x^n), \hat y^n)$ | $\frac 1 2 \sum_n (f(x^n) - \hat y^n)^2$ |
| update method | $w_i \leftarrow w_i - \eta \sum_n - (\hat y^n - f_{w, b}(x^n)) x_i^n$ | 

Cross entropy:

$$C(f(x^n), \hat y^n) = -[\hat y^n\ln f(x^n) + (1 - \hat y^n) \ln(1 - f(x^n))]$$

問題是：為什麼在 Logistic Regression 不用 rms 當 loss function 了？

答：做微分後，某些項次會為 $0$，導致參數更新過慢。

## Generative v.s. Discriminative

- Benefit of generative model
    - With the assumption of probability distribution,
less training data is needed
    - With the assumption of probability distribution, more robust to the noise
    - Priors and class-dependent probabilities can be estimated from different sources.