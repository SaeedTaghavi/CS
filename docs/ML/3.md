# Lecture 3: Gradient Descent

## Review: Gradient Descent

![](https://i.imgur.com/MGrouqh.png)

## Tip 1: Tuning your learning rates

### Adaptive Learning Rates

因為一開始 learning rate 更新的很慢，所以我們可以一開始先用較大的 learning rate 來加速，之後再逐漸下降 learning rate 來避免無法達到 loss 的 minimum，例如：$\eta^t = \eta / \sqrt{t + 1}$。

#### Adagrad

Adagrad 是一個常見的適性方法：Divide the learning rate of each parameter by the ***root mean square of its previous derivatives***

- Vanilla Gradient descent

    $$w^{t + 1} \leftarrow w^t - \eta^t g^t$$

- Adagrad

    \begin{align}
    w^{t + 1} & \leftarrow w^t - \frac{\eta^t}{\sigma^t} g^t \\\\
    w^{t + 1} & \leftarrow w^t - \frac{\eta}{\sqrt{\sum_{i = 0}^t (g^i)^2}} g^t
    \end{align}

其中，$\sigma^t$：***root mean square*** of the previous derivatives of parameter $w$

$g^t$ 代表 first derivative，分母則是利用歷史的 first derivative 去估計 second derivative。

## Tip 2: Stochastic Gradient Descent

$$L = \sum_n \Bigg(\hat y^n - \Big(b + \sum w_i x_i^n \Big) \Bigg)^2$$

SGD 能夠加速訓練，一般 GD 是在看到所有樣本（訓練數據）後，才計算出 $L$，再更新參數。而 SGD 是每看到一個樣本就計算一次 $L$，並更新參數。

## Tip 3: Feature Scaling

例如：$y = b + w_1x_1 + w_2x_2$

![](https://i.imgur.com/XDNSGri.png)

若不同參數的大小範圍差太多，那 loss function 等高線方向（負梯度、參數更新方向）不會指向 loss minimum。Feature scaling 讓不同參數的取相同區間範圍的值，變成正圓，更容易新參數。

做法如下，對 $x^1, x^2, x^3, \dots, x^r, \dots, x^R$ 每一個維度 $i$ 做標準化：

![](https://i.imgur.com/gpfr0f5.png)