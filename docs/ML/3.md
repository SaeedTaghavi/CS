# Lecture 3: Gradient Descent

## Review: Gradient Descent

![](https://i.imgur.com/MGrouqh.png)

在 Gradient Descent 中，我們想解決的是下列問題：

$$
\theta ^ { * } = \arg \min _ { \theta } L ( \theta )
$$

其中，

- $L$: loss function
- $\theta$: parameters

假設 $\theta$ 有兩個參數  ${\theta_1, \theta_2}$，我們隨機選取 $\theta ^ { 0 } = \left[ \begin{array} { l } { \theta _ { 1 } ^ { 0 } } \\\\ { \theta _ { 2 } ^ { 0 } } \end{array} \right]$，所以 $\nabla L ( \theta ) = \left[ \begin{array} { l } { \partial L \left( \theta _ { 1 } \right) / \partial \theta _ { 1 } } \\\\ { \partial L \left( \theta _ { 2 } \right) / \partial \theta _ { 2 } } \end{array} \right]$

\begin{align}
\left[ \begin{array} { l } { \theta _ { 1 } ^ { 1 } } \\\\ { \theta _ { 2 } ^ { 1 } } \end{array} \right] = \left[ \begin{array} { c } { \theta _ { 1 } ^ { 0 } } \\\\ { \theta _ { 2 } ^ { 0 } } \end{array} \right] - \eta \left[ \begin{array} { l } { \partial L \left( \theta _ { 1 } ^ { 0 } \right) / \partial \theta _ { 1 } } \\\\ { \partial L \left( \theta _ { 2 } ^ { 0 } \right) / \partial \theta _ { 2 } } \end{array} \right] \to \theta ^ { 1 } = \theta ^ { 0 } - \eta \nabla L \left( \theta ^ { 0 } \right) \\\\
\left[ \begin{array} { l } { \theta _ { 1 } ^ { 2 } } \\\\ { \theta _ { 2 } ^ { 2 } } \end{array} \right] = \left[ \begin{array} { c } { \theta _ { 1 } ^ { 1 } } \\\\ { \theta _ { 2 } ^ { 1 } } \end{array} \right] - \eta \left[ \begin{array} { l } { \partial L \left( \theta _ { 1 } ^ { 1 } \right) / \partial \theta _ { 1 } } \\\\ { \partial L \left( \theta _ { 2 } ^ { 1 } \right) / \partial \theta _ { 2 } } \end{array} \right] \to \theta ^ { 2 } = \theta ^ { 1 } - \eta \nabla L \left( \theta ^ { 1 } \right)
\end{align}

## Tip 1: Tuning your learning rates

我們必需要小心的設我們的 learning rate $\eta$

$$
\theta ^ { i } = \theta ^ { i - 1 } - \eta \nabla L \left( \theta ^ { i - 1 } \right)
$$

### Adaptive Learning Rates

因為一開始 learning rate 更新的很慢，所以我們可以一開始先用較大的 learning rate 來加速，之後再逐漸下降 learning rate 來避免無法達到 loss 的 minimum，例如：$\eta^t = \eta / \sqrt{t + 1}$。

#### Adagrad

Adagrad 是一個常見的適性方法：Divide the learning rate of each parameter by the ***root mean square of its previous derivatives***

- Vanilla Gradient descent

    $$w^{t + 1} \leftarrow w^t - \eta^t g^t$$

- Adagrad

    \begin{align}
    w^{t + 1} & \leftarrow w^t - \frac{\eta^t}{\sigma^t} g^t \\\\
    w^{t + 1} & \leftarrow w^t - \frac{\eta}{\sqrt{\sum_{i = 0}^t (g^i)^2}} g^t
    \end{align}

其中，$\sigma^t$：***root mean square*** of the previous derivatives of parameter $w$

$g^t$ 代表 first derivative，分母則是利用歷史的 first derivative 去估計 second derivative。

## Tip 2: Stochastic Gradient Descent

- Gradient Descent

    Loss is the summation over all training examples:

    $$L = \sum_n \Bigg(\hat y^n - \Big(b + \sum w_i x_i^n \Big) \Bigg)^2$$

    更新參數方式：

    $${ \theta } ^ { i } = { \theta } ^ { i - 1 } - { \eta } \nabla L \left( { \theta } ^ { i - 1 } \right)$$

- Stochastic Gradient Descent

    Pick an example $x^n$

    Loss for only one example:

    $$L ^ { n } = \left( \hat { y } ^ { n } - \left( b + \sum w _ { i } x _ { i } ^ { n } \right) \right) ^ { 2 }$$

    更新參數方式：

    $$\theta ^ { i } = \theta ^ { i - 1 } - \eta \nabla L ^ { n } \left( \theta ^ { i - 1 } \right)$$

Stochastic Gradient Descent 能夠加速訓練，一般 Gradient Descent 是在看到所有樣本（訓練數據）後，才計算出 $L$，再更新參數。而 Stochastic Gradient Descent 是每看到一個樣本就計算一次 $L$，並更新參數。

## Tip 3: Feature Scaling

例如：$y = b + w_1x_1 + w_2x_2$

![](https://i.imgur.com/XDNSGri.png)

若不同參數的大小範圍差太多，那 loss function 等高線方向（負梯度、參數更新方向）不會指向 loss minimum。Feature scaling 讓不同參數的取相同區間範圍的值，變成正圓，更容易新參數。

做法如下，對 $x^1, x^2, x^3, \dots, x^r, \dots, x^R$ 每一個維度 $i$ 做標準化：

![](https://i.imgur.com/gpfr0f5.png)