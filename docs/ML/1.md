# Lecture 1: Regression - Case Study

## Regression Steps

Machine Learning 有三個步驟：

1. 從一個集合中選出一個 function（即：選擇一個 model）
2. 評斷這個 function 的好壞
3. 選出 best function

Regression 的例子舉例有：

- 道瓊斯指數預測
- 自動駕駛中的方向盤角度預測
- 推薦系統中購買可能性的預測

而課程中的例子是預測寶可夢進化後的 CP 值。

一隻寶可夢可由 5 個參數表示：

$$x = (x_{cp}, x_s, x_{hp}, x_w, x_h).$$

步驟如下：

1. 選擇 linear model。
2. 評斷此 function 的好壞，它就好比函數的函數：我們輸入一個函數，輸出 how bad it is？這時就需要定義一個 loss function。在所選的 model 中，隨著參數不同，便有無數個 functions（即：model 確定之後，function 是由參數所決定的），每個 function 都有其對應的 loss。
3. 選擇 best function 即是選擇 loss 最小的 function（參數），求解最佳參數的方法可以是 gradient descent。

Gradient descent 的步驟是：

1. 選擇參數的初始值
2. 向損失函數對參數的負梯度方向迭代更新
3. learning rate 控制一次走多遠、學習速度

Gradient descent 可能使參數停在損失函數的局部最小值、微分為 $0$ 的點，或者微分為極小值的點。Linear regression 中不必擔心局部最小值的問題，因為 loss function 是 convex 的。

在得到 best function 之後，我們真正在意的是它在 testing data 上的表現。選擇不同的 model，會得到不同的 best function，它們在testing data 上有不同表現。

複雜模型的 model space 涵蓋了簡單模型的 model space，因此在 training data 上的錯誤率更小，但並不意味著在 testing data 上錯誤率更小。Model 太複雜會出現 overfitting。

## What are the hidden factors?

如果我們收集更多寶可夢進化前後的 CP 值會發現，進化後的 CP 值不只依賴於進化前的 CP 值，可能還有其它的隱藏因素（例如：寶可夢所屬的物種）。同時考慮進化前 CP 值 $x_{cp}$ 和物種 $x_s$，之前的模型要修改為：

\begin{align}
\text{if } & x_s = \text{pidgey}:   & y = b_1 + w_1 \cdot x_{cp} \\\\
\text{if } & x_s = \text{weedle}:   & y = b_2 + w_2 \cdot x_{cp} \\\\
\text{if } & x_s = \text{caterpie}: & y = b_3 + w_3 \cdot x_{cp} \\\\
\text{if } & x_s = \text{eevee}:    & y = b_4 + w_4 \cdot x_{cp}
\end{align}

這仍是一個線性模型，因為它可以寫作：

\begin{align}
y = & b_1 \cdot \boldsymbol{\delta(x_s = \text{pidgey})  } & + & w_1 \cdot \boldsymbol{\delta(x_s = \text{pidgey})   \cdot x_{cp}} + \\\\
    & b_2 \cdot \boldsymbol{\delta(x_s = \text{weedle})  } & + & w_2 \cdot \boldsymbol{\delta(x_s = \text{weedle})   \cdot x_{cp}} + \\\\
    & b_3 \cdot \boldsymbol{\delta(x_s = \text{caterpie})} & + & w_3 \cdot \boldsymbol{\delta(x_s = \text{caterpie}) \cdot x_{cp}} + \\\\
    & b_4 \cdot \boldsymbol{\delta(x_s = \text{eevee})   } & + & w_4 \cdot \boldsymbol{\delta(x_s = \text{eevee})    \cdot x_{cp}}
\end{align}

上式中的粗體項都是 linear model $y = b + \Sigma w_i \cdot x_i$ 中的 feature $x_i$。

這個模型在 testing data 上有更好的表現，如果同時考慮寶可夢的其它屬性，選一個很複雜的模型，結果可能會導致 overfitting。

對線性模型來說，我們希望選出的 best function 能平滑一點，也就是權重係數小一些，因為這樣的話，在 testing data 受 noise 影響時，預測值所受的影響會更小。

所以在 loss function 中加一個正則項 $\lambda\Sigma w_i^2$。

越大的 $\lambda$，training error 可能會越大，但沒關係，我們希望函數平滑，但也不能太平滑，於是我們調整 $\lambda$，選擇使 testing error 最小的 $\lambda$。
