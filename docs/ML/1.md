# Lecture 1: Regression - Case Study

machine learning 有三个步骤，step 1 是选择 a set of function, 即选择一个 model，step 2 是评价goodness of function，step 3 是选出 best function。

regression 的例子有道琼斯指数预测、自动驾驶中的方向盘角度预测，以及推荐系统中购买可能性的预测。课程中的例子是预测宝可梦进化后的CP值。

一只宝可梦可由5个参数表示，x=(x_cp, x_s, x_hp, x_w, x_h)。我们在选择 model 的时候先选择linear model。接下来评价goodness of function ，它类似于函数的函数，我们输入一个函数，输出的是how bad it is，这就需要定义一个loss function。在所选的model中，随着参数的不同，有着无数个function（即，model确定之后，function是由参数所决定的），每个function都有其loss，选择best function即是选择loss最小的function（参数），求解最优参数的方法可以是gradient descent。

gradient descent 的步骤是：先选择参数的初始值，再向损失函数对参数的负梯度方向迭代更新，learning rate控制步子大小、学习速度。梯度方向是损失函数等高线的法线方向。

gradient descent 可能使参数停在损失函数的局部最小值、导数为0的点、或者导数极小的点处。线性回归中不必担心局部最小值的问题，损失函数是凸的。

在得到best function之后，我们真正在意的是它在testing data上的表现。选择不同的model，会得到不同 的best function，它们在testing data 上有不同表现。复杂模型的model space涵盖了简单模型的model space，因此在training data上的错误率更小，但并不意味着在testing data 上错误率更小。模型太复杂会出现overfitting。

如果我们收集更多宝可梦进化前后的CP值会发现，进化后的CP值不只依赖于进化前的CP值，还有其它的隐藏因素（比如宝可梦所属的物种）。同时考虑进化前CP值x_cp和物种x_s，之前的模型要修改为 
if  x_s = pidgey:   y = b_1 + w_1 * x_cp 
if  x_s = weedle:   y = b_2 + w_2 * x_cp 
if  x_s = caterpie: y = b_3 + w_3 * x_cp 
if  x_s = eevee:    y = b_4 + w_4 * x_cp 
这仍是一个线性模型，因为它可以写作 
y =  
b_1 * δ(x_s = pidgey)    + w_1 * δ(x_s = pidgey) * x_cp + 
 b_2 * δ(x_s = weedle)   + w_2 * δ(x_s = weedle) * x_cp +  
 b_3 * δ(x_s = caterpie) + w_3 * δ(x_s = caterpie) * x_cp + 
  b_4 * δ(x_s = eevee) + w_4 * δ(x_s = eevee) * x_cp 
上式中的粗体项都是linear model y = b + Σw_i * x_i 中的feature x_i。 
这个模型在测试集上有更好的表现。如果同时考虑宝可梦的其它属性，选一个很复杂的模型，结果会overfitting。

对线性模型来讲，希望选出的best function 能 smooth一些，也就是权重系数小一些，因为这样的话，在测试数据受噪声影响时，预测值所受的影响会更小。 
所以在损失函数中加一个正则项 λΣ(w_i)^2。 
越大的λ，对training error考虑得越少。 
我们希望函数smooth，但也不能太smooth。 
调整λ，选择使testing error最小的λ。





---------------------

本文来自 徐子尧 的CSDN 博客 ，全文地址请点击：https://blog.csdn.net/xzy_thu/article/details/67640512?utm_source=copy 